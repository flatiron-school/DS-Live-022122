{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Intro\" data-toc-modified-id=\"Intro-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Intro</a></span><ul class=\"toc-item\"><li><span><a href=\"#Two-Types\" data-toc-modified-id=\"Two-Types-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Two Types</a></span></li></ul></li><li><span><a href=\"#Create-Some-Noisy-Data\" data-toc-modified-id=\"Create-Some-Noisy-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create Some Noisy Data</a></span></li><li><span><a href=\"#AdaBoost-(Adaptive-Boosting)\" data-toc-modified-id=\"AdaBoost-(Adaptive-Boosting)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>AdaBoost (Adaptive Boosting)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Algorithm\" data-toc-modified-id=\"Algorithm-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Algorithm</a></span></li><li><span><a href=\"#Voting\" data-toc-modified-id=\"Voting-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Voting</a></span></li><li><span><a href=\"#AdaBoost-in-Scikit-Learn\" data-toc-modified-id=\"AdaBoost-in-Scikit-Learn-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>AdaBoost in Scikit-Learn</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-4.3.0.1\"><span class=\"toc-item-num\">4.3.0.1&nbsp;&nbsp;</span>Hyperparameters</a></span></li></ul></li><li><span><a href=\"#Galaxy-Data\" data-toc-modified-id=\"Galaxy-Data-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Galaxy Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-4.3.1.1\"><span class=\"toc-item-num\">4.3.1.1&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Gradient Boosting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Algorithm\" data-toc-modified-id=\"Algorithm-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Algorithm</a></span></li><li><span><a href=\"#Example-of-Iterative-Steps\" data-toc-modified-id=\"Example-of-Iterative-Steps-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Example of Iterative Steps</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recall-our-noisy-data-from-earlier\" data-toc-modified-id=\"Recall-our-noisy-data-from-earlier-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Recall our noisy data from earlier</a></span></li><li><span><a href=\"#Train-iteratively-on-the-residuals-of-its-predecessor\" data-toc-modified-id=\"Train-iteratively-on-the-residuals-of-its-predecessor-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Train iteratively on the residuals of its predecessor</a></span></li><li><span><a href=\"#Observe-how-the-regressor-gets-better\" data-toc-modified-id=\"Observe-how-the-regressor-gets-better-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Observe how the regressor gets better</a></span></li><li><span><a href=\"#Using-SciKit-learn's-Gradient-Boosting\" data-toc-modified-id=\"Using-SciKit-learn's-Gradient-Boosting-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Using SciKit-learn's Gradient Boosting</a></span></li></ul></li><li><span><a href=\"#Comparing-gradient-boosting-with-many-estimators\" data-toc-modified-id=\"Comparing-gradient-boosting-with-many-estimators-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Comparing gradient boosting with many estimators</a></span></li></ul></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>XGBoost</a></span><ul class=\"toc-item\"><li><span><a href=\"#XGBoost-Regression\" data-toc-modified-id=\"XGBoost-Regression-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>XGBoost Regression</a></span></li></ul></li><li><span><a href=\"#Level-Up:-Regression-or-Classification?\" data-toc-modified-id=\"Level-Up:-Regression-or-Classification?-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Level Up: Regression or Classification?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adaboost-Classification\" data-toc-modified-id=\"Adaboost-Classification-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Adaboost Classification</a></span></li><li><span><a href=\"#GradientBoosting\" data-toc-modified-id=\"GradientBoosting-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>GradientBoosting</a></span></li><li><span><a href=\"#XGBoost-Classification\" data-toc-modified-id=\"XGBoost-Classification-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>XGBoost Classification</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, \\\n",
    "AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe boosting algorithms\n",
    "- Implement boosting models with `sklearn` \n",
    "- Implement boosting models with `XGBoost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with using single decision trees and random forests is that, once I make a split, I can't go back and consider how another feature varies across the whole dataset. But suppose I were to consider **my tree's errors**. The fundamental idea of ***boosting*** is to start with a weak learner and then to use information about its errors to build a new model that can supplement the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main types of boosting available in Scikit-Learn are adaptive boosting (AdaBoostClassifier, AdaBoostRegressor) and gradient boosting (GradientBoostingClassifier, GradientBoostingRegressor).\n",
    "\n",
    "Again, the fundamental idea of boosting is to use a sequence of **weak** learners to build a model. Though the individual learners are weak, the idea is to train iteratively in order to produce a better predictor. More specifically, the first learner will be trained on the data as it stands, but future learners will be trained on modified versions of the data. The point of the modifications is to highlight the \"hard-to-predict-accurately\" portions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Some Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "X = np.random.rand(n, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(n)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax0,ax1) = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ax0.scatter(X_train, y_train, alpha=0.3)\n",
    "ax1.scatter(X_test, y_test, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **AdaBoost** works by iteratively adapting two related series of weights, one attached to the datapoints and the other attached to the learners themselves. Datapoints that are incorrectly classified receive greater weights for the next learner in the sequence. That way, future learners will be more likely to focus on those datapoints. At the end of the sequence, the learners that make better predictions, especially on the datapoints that are more resistant to correct classification, receive more weight in the final \"vote\" that determines the ensemble's prediction. <br/> Suppose we have a binary classification problem and we represent the two classes with 1 and -1. (This is standard for describing the algorithm of AdaBoost.) <br/>\n",
    "Then, in a nutshell: <br/>\n",
    "    1. Train a weak learner. <br/>\n",
    "    2. Calculate its error $\\epsilon$. <br/>\n",
    "    3. Use that error as a weight on the classifier: $\\theta = \\frac{1}{2}ln\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)$. <br/>\n",
    "    Note that $\\theta$ CAN be negative. This represents a classifier whose accuracy is _worse_ than chance. <br/>\n",
    "    4. Use _that_ to adjust the data points' weights: $w_{n+1} = w_n\\left(\\frac{e^{\\pm\\theta}}{scaler}\\right)$. Use $+\\theta$ for incorrect predictions, $-\\theta$ for correct predictions. <br/>  $\\rightarrow$ For more detail on AdaBoost, see [here](https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train model\n",
    "- Inflate errors\n",
    "- Retrain model using errors (repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/adaboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Vote by combining these calculated $y$ for each crossed-area (negative for \"not blue\" or whatever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=4), n_estimators = 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`base_estimator`: The model utilized for the weak learners (Warning: Don't forget to import the model that you decide to use for the weak learner).\n",
    "`n_estimators`: The maximum number of weak learners used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Galaxy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = pd.read_csv('data/COMBO17.csv')\n",
    "galaxies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset about galaxies. The Mcz and MCzml columns are measures of redshift, which is our target. Mcz is usually understood to be a better measure, so that will be our target column. Many of the other columns have to do with various measures of galaxies' magnitudes. For more on the dataset, see [here](https://astrostatistics.psu.edu/datasets/COMBO17.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "galaxies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = galaxies.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect together the columns that have high correlation with Mcz, our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for ind in galaxies.corr()['Mcz'].index:\n",
    "    if abs(galaxies.corr()['Mcz'][ind]) > 0.5:\n",
    "        preds.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = galaxies[preds].corr()\n",
    "\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "sns.heatmap(corr, mask=mask, annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These various magnitude columns all have high correlations **with one another**! Let's try a simple model with just the MCzml column, since it has the highest correlation with Mcz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_x = galaxies['MCzml']\n",
    "galaxies_y = galaxies['Mcz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have one predictor, we can visualize the correlation with the target! We can also reshape it for modeling purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_x_rev = galaxies_x.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(galaxies_x_rev, galaxies_y, alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_x_train, galaxies_x_test, galaxies_y_train, galaxies_y_test =\\\n",
    "train_test_split(galaxies_x_rev, galaxies_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better by trying different hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID IT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use gradient descent to improve the model\n",
    "\n",
    "![](images/gradient_boosting_residuals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Boosting** works instead by training each new learner on the residuals of the model built with the learners that have so far been constructed. That is, Model $n+1$ (with $n+1$ learners) will focus on the predictions of Model $n$ (with only $n$ learners) that were **most off the mark**. As the training process repeats, the learners learn and the residuals get smaller. I would get a sequence going: a<br/> Model 0 is very simple. Perhaps it merely predicts the mean: <br/>\n",
    "$\\hat{y}_0 = \\bar{y}$; <br/>\n",
    "Model 1's predictions would then be the sum of (i) Model 0's predictions and (ii) the predictions of the model fitted to Model 0's residuals: <br/> $\\hat{y}_1 = \\hat{y}_0 + \\hat{(y - \\hat{y})}_{err0}$; <br/>\n",
    "Now iterate: Model 2's predictions will be the sum of (i) Model 0's predictions, (ii) the predictions of the model fitted to Model 0's residuals, and (iii) the predictions of the model fitted to Model 1's residuals: <br/> $\\hat{y}_2 = \\hat{y}_0 + \\hat{(y - \\hat{y})}_{err0} + \\hat{(y - \\hat{y})}_{err1}$<br/>\n",
    "Etc.\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ How does gradient boosting work for a classification problem? How do we even make sense of the notion of a gradient in that context? The short answer is that we appeal to the probabilities associated with the predictions for the various classes. See more on this topic [here](https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/). <br/> $\\rightarrow$ Why is this called \"_gradient_ boosting\"? Because using a model's residuals to build a new model is using information about the derivative of that model's loss function. See more on this topic [here](https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mean squared error (MSE) and want to minimize that <-- done by gradient descent\n",
    "\n",
    "Use the residuals (pattern in the residuals) to create an even better model\n",
    "\n",
    "1. Fit a model to the data, $F_1(x) = y$\n",
    "2. Fit a model to the residuals, $h_1(x) = y - F_1(x)$\n",
    "3. Create a new model, $F_2(x) = F_1(x) + h_1(x)$\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Iterative Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Parts adapted from https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall our noisy data from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ax0.scatter(X_train, y_train, alpha=0.3)\n",
    "ax1.scatter(X_test, y_test, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12,4))\n",
    "ax0.scatter(X, y, alpha=0.3)\n",
    "ax1.scatter(X_train, y_train, alpha=0.3)\n",
    "ax2.scatter(X_test, y_test, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train iteratively on the residuals of its predecessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First iteration\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=seed)\n",
    "tree_reg1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Second iteration\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=seed)\n",
    "tree_reg2.fit(X_train, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that will repeat this process for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gradient_boosting_regressor(n_estimators, X, y, random_state=27):\n",
    "    # Save the iteratively trained models and residuals\n",
    "    trained_weak_learners = []\n",
    "    past_residuals = []\n",
    "    \n",
    "    # Initial conditions\n",
    "    new_y = y\n",
    "    past_residuals.append(new_y)\n",
    "    # Iteratively train model\n",
    "    for n in range(n_estimators):\n",
    "        # Train the new model on past residuals (note first model trains on y)\n",
    "        new_model = DecisionTreeRegressor(max_depth=2, random_state=random_state)\n",
    "        new_model.fit(X, new_y)\n",
    "        # Find the new residuals (used to train the next model)\n",
    "        new_y = new_y - new_model.predict(X) \n",
    "        # Save the (trained) model and the new residuals\n",
    "        trained_weak_learners.append(new_model)\n",
    "        past_residuals.append(new_y)\n",
    "        \n",
    "    return trained_weak_learners, past_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, ys = my_gradient_boosting_regressor(3, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe how the regressor gets better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(regressors, X, y, axes, label=None, style='r-', data_style='b.',\n",
    "               data_label=None, ax=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    ax.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    ax.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        ax.legend(loc='upper center')\n",
    "\n",
    "def gradient_boost_and_plot(n_estimators, X, y):\n",
    "    \n",
    "    fig_size = (16, 4*n_estimators)\n",
    "    model_list, resid_list = my_gradient_boosting_regressor(16, X, y)\n",
    "    f, axes = plt.subplots(nrows=n_estimators, ncols=2, figsize=fig_size)\n",
    "\n",
    "    base_label ='h(x_1) = '\n",
    "    iterative_label = 'y'\n",
    "    for i in range(n_estimators):\n",
    "        # Next set of residuals\n",
    "        y = resid_list[i]\n",
    "        # Must be a list of one item\n",
    "        past_model = [model_list[i]]\n",
    "        # Includes current model\n",
    "        past_models = model_list[:(i+1)]\n",
    "        # New labels\n",
    "        new_label = f'h_{i}(x_{i})'\n",
    "        base_label = f'{base_label} + {new_label}'\n",
    "\n",
    "        plot_preds(\n",
    "            past_model,\n",
    "            X,\n",
    "            y,\n",
    "            axes=[-0.5, 0.5, -0.1, 0.8], \n",
    "            label=f'${iterative_label}$',\n",
    "            style=\"g-\",\n",
    "            data_label='Training set',\n",
    "            ax=axes[i][0]\n",
    "        )\n",
    "        axes[i][0].set_title('Residuals and tree predictions')\n",
    "\n",
    "        plot_preds(\n",
    "            past_models,\n",
    "            X,\n",
    "            resid_list[0],\n",
    "            axes=[-0.5, 0.5, -0.1, 0.8],\n",
    "            label=f'${base_label}$',\n",
    "            data_label='Training set',\n",
    "            ax=axes[i][1]\n",
    "        )\n",
    "        axes[i][1].set_ylabel('$y$', fontsize=16, rotation=0)\n",
    "        axes[i][1].set_title('Ensemble predictions')\n",
    "\n",
    "        # Update labels for next round\n",
    "        base_label = f'{base_label} + '\n",
    "        iterative_label = f'{iterative_label} - {new_label}'\n",
    "        \n",
    "    return f, model_list, resid_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gradient_boost_and_plot(3, X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SciKit-learn's Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradient boosting with many estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More estimators\n",
    "\n",
    "\n",
    "# Even more estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "\n",
    "axes = [-0.5, 0.5, -0.1, 0.8]\n",
    "plot_preds([gbrt], X, y, axes=axes, ax=ax0)\n",
    "ax0.set_title(\n",
    "    f\"learning_rate={gbrt.learning_rate}, n_estimators={gbrt.n_estimators}\",\n",
    "              fontsize=14)\n",
    "\n",
    "plot_preds([gbrt_slow], X, y, axes=axes, ax=ax1)\n",
    "ax1.set_title(\n",
    "    f\"learning_rate={gbrt_slow.learning_rate}, n_estimators={gbrt_slow.n_estimators}\",\n",
    "              fontsize=14)\n",
    "\n",
    "plot_preds([gbrt_slower], X, y, axes=axes, ax=ax2)\n",
    "ax2.set_title(\n",
    "    f\"learning_rate={gbrt_slower.learning_rate}, n_estimators={gbrt_slower.n_estimators}\",\n",
    "              fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt.score(X_train, y_train), gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_slow.score(X_train, y_train), gbrt_slow.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_slower.score(X_train, y_train), gbrt_slower.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [XGBoost's documentation](https://xgboost.readthedocs.io/):\n",
    "\n",
    ">_**XGBoost** is an optimized distributed gradient boosting library designed to be highly **efficient**, **flexible** and **portable**. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Regression or Classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does my target look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "galaxies['Mcz'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a bit of a bimodal shape here. We might therefore try predicting whether the redshift factor is likely to be greater or less than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies['bool'] = galaxies['Mcz'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "galaxies.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(galaxies_x_rev, galaxies['bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(estimator, X, y):\n",
    "    '''\n",
    "    Prints the accuracy, precision, and recall of the given fitted estimator and data.\n",
    "    \n",
    "    Prints the confusion matrix.\n",
    "    '''\n",
    "    p_score = precision_score(y, estimator.predict(X))\n",
    "    r_score = recall_score(y, estimator.predict(X))\n",
    "    \n",
    "    print(f'Accuracy: {estimator.score(X, y)}')\n",
    "    print(f'Precision: {p_score}')\n",
    "    print(f'Recall: {r_score}')\n",
    "    \n",
    "    plot_confusion_matrix(estimator, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GadBoostClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "884px",
    "left": "120px",
    "top": "146px",
    "width": "341px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
