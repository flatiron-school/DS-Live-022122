{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#What-is-Big-Data?\" data-toc-modified-id=\"What-is-Big-Data?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What is Big Data?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Numbers-Everyone-Should-Know\" data-toc-modified-id=\"Numbers-Everyone-Should-Know-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Numbers Everyone Should Know</a></span></li><li><span><a href=\"#The-3-Vs-of-Data\" data-toc-modified-id=\"The-3-Vs-of-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The 3 Vs of Data</a></span></li></ul></li><li><span><a href=\"#Applying-it-via-Tools\" data-toc-modified-id=\"Applying-it-via-Tools-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Applying it via Tools</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hadoop-Framework\" data-toc-modified-id=\"Hadoop-Framework-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Hadoop Framework</a></span></li><li><span><a href=\"#Apache-Spark\" data-toc-modified-id=\"Apache-Spark-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Apache Spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-Data-Objects\" data-toc-modified-id=\"Spark-Data-Objects-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Spark Data Objects</a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-an-RDD-when:\" data-toc-modified-id=\"Use-an-RDD-when:-3.2.1.1\"><span class=\"toc-item-num\">3.2.1.1&nbsp;&nbsp;</span>Use an RDD when:</a></span></li><li><span><a href=\"#Use-a-dataframe-when:\" data-toc-modified-id=\"Use-a-dataframe-when:-3.2.1.2\"><span class=\"toc-item-num\">3.2.1.2&nbsp;&nbsp;</span>Use a dataframe when:</a></span></li></ul></li></ul></li><li><span><a href=\"#But-Spark-Isn't-Always-the-Best-Tool!\" data-toc-modified-id=\"But-Spark-Isn't-Always-the-Best-Tool!-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>But Spark Isn't Always the Best Tool!</a></span></li></ul></li><li><span><a href=\"#What-Do-We-Mean-by-&quot;Parallel&quot;-&amp;-&quot;Distributed&quot;?\" data-toc-modified-id=\"What-Do-We-Mean-by-&quot;Parallel&quot;-&amp;-&quot;Distributed&quot;?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>What Do We Mean by \"Parallel\" &amp; \"Distributed\"?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Distributed\" data-toc-modified-id=\"Distributed-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Distributed</a></span></li><li><span><a href=\"#Sequential\" data-toc-modified-id=\"Sequential-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Sequential</a></span></li><li><span><a href=\"#Parallel\" data-toc-modified-id=\"Parallel-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Parallel</a></span></li></ul></li><li><span><a href=\"#MapReduce\" data-toc-modified-id=\"MapReduce-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>MapReduce</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steps-in-MapReduce\" data-toc-modified-id=\"Steps-in-MapReduce-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Steps in MapReduce</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split\" data-toc-modified-id=\"Split-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Split</a></span></li><li><span><a href=\"#Map\" data-toc-modified-id=\"Map-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Map</a></span></li><li><span><a href=\"#Shuffle\" data-toc-modified-id=\"Shuffle-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Shuffle</a></span></li><li><span><a href=\"#Reduce\" data-toc-modified-id=\"Reduce-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Reduce</a></span></li></ul></li><li><span><a href=\"#MapReduce-Examples-with-mrjob\" data-toc-modified-id=\"MapReduce-Examples-with-mrjob-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>MapReduce Examples with <code>mrjob</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Song-Plays\" data-toc-modified-id=\"Song-Plays-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Song Plays</a></span></li><li><span><a href=\"#Word-Count\" data-toc-modified-id=\"Word-Count-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Word Count</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Understand what is \"big data\"\n",
    "- Know why big data is different and how it can be processed\n",
    "- Understand how data can be handled in distributed and parallel systems\n",
    "- Understand how MapReduce is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What is Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> A different amount makes a different kind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is no clear/agreed upon definition but typically we say we're working on **big data** if we have to use something like a distributed computing system (not just one local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Numbers Everyone Should Know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To determine big data, we should consider the **\"numbers everyone should know\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- CPU\n",
    "    + Brain\n",
    "    + 0.5 ns\n",
    "- Memory (RAM)\n",
    "    + Short-term memory\n",
    "    + 100 ns\n",
    "    + ~ 200x slower than CPU\n",
    "- Storage (SSD/HDD)\n",
    "    + Long-term memory\n",
    "    + 15 µs / 800 µs (15,000 ns / 8,000,000 ns)\n",
    "    + ~ 15x slower than memory\n",
    "- Network\n",
    "    + Connecting machines\n",
    "    + 150 ms (150,000,000 ns)\n",
    "    + ~ 20x slower than storage\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    \n",
    "Originally Jeff Dean & Peter Norvig; original blog: http://norvig.com/21-days.html#Answers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Interactive numbers: https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Thrashing**: when your CPU is bored, waiting for tasks to do since it has to wait for their slowpoke friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The 3 Vs of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "+ Volume --> Large Amounts\n",
    "+ Velocity --> Quickly Generated\n",
    "+ Variety --> Unstructured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/3vs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data is *big* when it is better/faster to split the work over the network amongst more (parallel) because of one or more of these Vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Applying it via Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hadoop Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/hadoop_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Considered \"old-school\"\n",
    ">\n",
    "> Slower since it has to write to disk each time\n",
    "\n",
    "- Storage (usually [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)) \n",
    "- Data Processing (MapReduce)\n",
    "- Resource Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/apache_spark_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Holds data in memory whenever possible (faster)\n",
    ">\n",
    "> Can still be built on top of Hadoop but also S3 on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spark has become king of data since it does a good job with ETL (Extract-Transform-Load) & ML in distributed systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### _Aside: More Detail on Spark_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Some Resources**\n",
    "\n",
    ">[Here](https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427) is a great walkthrough of Spark basics!\n",
    ">\n",
    "> And [here](https://towardsdatascience.com/apache-spark-a-conceptual-orientation-e326f8c57a64)'s another from our very own alum, Alex Shropshire!\n",
    ">\n",
    "> Spark has APIs for Scala (this is ur-Spark), Java, Python, and R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "N.B. Unless otherwise marked, page references are to [Salloum, Dautov, et al., \"Big Data Analytics on Apache Spark\", 2016](https://link.springer.com/content/pdf/10.1007%2Fs41060-016-0027-9.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spark is a tool for the management of big data. Sometimes data science professionals will refer to the [five \"V\"s](https://www.bbva.com/en/five-vs-big-data/) of big data. Clearly, the availabilty and size of datasets are growing rapidly. What counts as \"big data\"? Roughly speaking, we're talking about datasets that are too large to be processed on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many large companies are relying on big data these days, and Spark is a major player in the big data game. Examples can be found [here](https://www.icas.com/thought-leadership/technology/10-companies-using-big-data) and [here](https://enlyft.com/tech/products/apache-spark) and [here](https://www.quora.com/Which-are-the-companies-that-use-apache-spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So ... how in the world *do* you process a dataset that's too large for a single machine? You use multiple machines linked together! Let's call each machine a *node*, and the group of all machines working in parallel a *cluster*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The origin story of Spark starts with [MapReduce](https://en.wikipedia.org/wiki/MapReduce), whose programs comprise (unsurprisingly) a \"map\" routine (for filtering and sorting) and a \"reduce\" routine (for performing some aggregate operation).\n",
    "\n",
    "Let's look at an [example](https://en.wikipedia.org/wiki/MapReduce#Logical_view):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An early major player in big data that used MapReduce was [Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop). Hadoop was (and still is) a framework for distributed data processing. Its processing component used MapReduce, but it also had a storage component called the \"Hadoop Distributed File System\".\n",
    "\n",
    "From Wikipedia: \"Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel. This approach takes advantage of data locality, where nodes manipulate the data they have access to. This allows the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But Spark appeared as open source in 2010, and had some advantages over Hadoop MapReduce.\n",
    "\n",
    "Spark's advances over Hadoop MapReduce:\n",
    "\n",
    "- data processing in memory rather than on disks\n",
    "- a single framework for machine learning, graph analysis, and processing of streaming data (pp. 159-160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For more on the advantages of Spark over MapReduce, see [this piece](https://research.ijcaonline.org/volume113/number1/pxc3900531.pdf).\n",
    "\n",
    "Distributed computing can help enormously with speed. Check out [this website](http://sortbenchmark.org) for the latest in speed records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\"As a framework, it combines a core engine for distributed computing with an advanced programming model for in-memory processing. Although it has the same linear scalability and fault tolerance capabilities as those of MapReduce, it comes with a multistage in-memory programming model comparing to the rigid map-then-reduce disk-based model\" (146).\n",
    "\n",
    "Illustration, p. 148, of Spark guts.\n",
    "\n",
    "\"Running a Spark application involves five key entities ... a driver program, a cluster manager, workers, executors and tasks. A driver program is an application that uses Spark as a library and defines a high-level control flow of the target computation. While a worker provides CPU, memory and storage resources to a Spark application, an executer \\[sic\\] is a JVM (Java Virtual Machine) process that Spark creates on each worker for that application. A job is a set of computations (e.g., a data processing algorithm) that Spark performs on a cluster to get results to the driver program. A Spark application can launch multiple jobs. Spark splits a job into a directed acyclic graph (DAG) of stages where each stage is a collection of tasks. A task is the smallest unit of work that Spark sends to an executor. The main entry point for Spark functionalities is a SparkContext through which the driver program access \\[sic\\] Spark. A SparkContext represents a connection to a computing cluster\" (149)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[RDD](https://databricks.com/glossary/what-is-rdd)s, Transformations, and Actions:\n",
    "\n",
    "Fault tolerance achieved by keeping a record of the RDD's lineage. There are *redundancies* in the data records, so that, in the event of node failure, the other nodes can provide for data recovery. This is what makes these RDDs *resilient*.\n",
    "\n",
    "- Transformations take one from an RDD to another RDD;\n",
    "- Actions take one from an RDD to an output value.\n",
    "\n",
    "Broadcast variables and accumulators act as global variables; the latter are for counters or sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Surveys of Big Data tools [here](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0032-1) and [here](https://ieeexplore.ieee.org/document/7300948).\n",
    "\n",
    "Debugging can be a challenge in Spark. [This project](https://sites.google.com/site/sparkbigdebug/) was started to help with that.\n",
    "\n",
    "Also check out Paco Nathan's [massive slide show presentation](http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf) on Spark. Let's just look at slides 66-7 and 82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Aside: The story of Spark (a timeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|<p align=\"left justify\">Date</p>|<p align=\"left justify\">Product</p>|<p align=\"left justify\">Update</p>|\n",
    "|:----|:-----|:-----|\n",
    "| 2002 | Hadoop | <p align=\"left justify\">Doug Cutting starts `Apache Nutch` researching sort/merge processing</p> |\n",
    "| 2006 | Hadoop |  <p align=\"left justify\">Leaves `Nutch` and joins `Yahoo`, renaming the project `Hadoop` </p>|\n",
    "| 2008 | Hadoop |  <p align=\"left justify\">`Hadoop` was made `Apache’s` top level project </p> |\n",
    "| Jan 2008 | Hadoop |  <p align=\"left justify\">v 0.10.1 released </p>|\n",
    "| 2009 | Spark | <p align=\"left justify\">started as a research project at the UC Berkeley AMPLab  </p>|\n",
    "| 2010 | Spark |  <p align=\"left justify\">open sourced </p>|\n",
    "| Sept 2012 | Spark |  <p align=\"left justify\">0.6.0 released </p>|\n",
    "| 2013 | Spark |  <p align=\"left justify\">moved to the `Apache` Software Foundation </p>|\n",
    "| Feb 2013| Spark |  <p align=\"left justify\">Spark 0.7 adds a Python API called `PySpark` </p>|\n",
    "| Sept 2013 | Spark | <p align=\"left justify\">0.8.0 introduces `MLlib` </p>|\n",
    "| 2013 | Databricks |  <p align=\"left justify\">Original Spark research team at UC Berkeley found Databricks</p> |\n",
    "| May 2014 |Spark |  <p align=\"left justify\">v 1.0 introduces Spark SQL, for loading and manipulating structured data in Spark</p>|\n",
    "| Sept 2014 | Spark|  <p align=\"left justify\">v 1.1.0 provided support for registering Python lambda funtions as UDFs</p>|\n",
    "|Mar 2015 | Spark | <p align=\"left justify\"> v 1.3.0 brings a new DataFrame API</p> |\n",
    "| Jun 2015 | Spark | <p align=\"left justify\"> v 1.4.0 brings an R API to Spark</p> |\n",
    "| 2015 | Databricks | <p align=\"left justify\"> The Databricks Apache Spark cloud platform goes public</p> |\n",
    "| Jan 2016|  Spark | <p align=\"left justify\"> v 1.6.0 brings a new Dataset API <br> - A new Spark API, similar to RDDs, that allows users to work with custom objects and lambda functions while still gaining the benefits of the Spark SQL execution engine.</p> |\n",
    "| Jul 2016 | Spark | <p align=\"left justify\"> v 2.0.0 **big update**! <Br> - Unifying DataFrame and Dataset: In Scala and Java, DataFrame and Dataset have been unified, i.e. DataFrame is just a type alias for Dataset of Row. In Python and R, given the lack of type safety, DataFrame is the main programming interface. <br> - SparkSession: new entry point that replaces the old SQLContext<br>- Native CSV data source, based on Databricks’ spark-csv module<br>- MLlib - The DataFrame-based API is now the primary API. The RDD-based API is entering maintenance mode </p> |\n",
    "| 2016 | Databricks | <p align=\"left justify\"> Databricks Launches Free Community Edition As Companion To Free Online Spark Courses </p>|\n",
    "| Jul 2017| Spark | <p align=\"left justify\"> v 2.2.0 drops support for Python 2.6 |\n",
    "| Nov 2018 | Spark | <p align=\"left justify\"> v 2.4.0<br> - This release adds Barrier Execution Mode for better integration with deep learning frameworks<br> - more integration between pandas UDF and spark DataFrames </p>|\n",
    "| May 2021 | Spark | 2.4 LTS|\n",
    "| June 2020 | Spark | 3.0|\n",
    "| March 2021 | Spark| 3.1.2 |\n",
    "| October 2021 | Spark | 3.2.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Spark Data Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***In Pyspark there are only RDD and DataFrames***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In other languages where \"compiling\" is done, there is the distinction between DataFrames and DataSet. \n",
    "\n",
    "![dataframe image](https://databricks.com/wp-content/uploads/2018/05/DataFrames.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Use an RDD when:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "> - you want low-level transformation and actions and control on your dataset;\n",
    "> - your data is unstructured, such as media streams or streams of text;\n",
    "> - you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Use a dataframe when:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[also quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> - you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
    "> - your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
    "> - you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - If you are a R user, use DataFrames.\n",
    "> - If you are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note**: Machine learning algorithms are run on _DataFrames_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## But Spark Isn't Always the Best Tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/tech_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What Do We Mean by \"Parallel\" & \"Distributed\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/types_of_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> tasks split up and executed by different workers\n",
    "\n",
    "+ Multiple CPUs each have their own memory\n",
    "+ Multiple CPUs share via a network (using \"messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/sequential.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Take a step at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> executing tasks in a non-sequential order\n",
    "\n",
    "+ Multiple CPUs share same memory to \"communicate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Describes two jobs: **Map** & **Reduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Software best for **clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Below explain each step of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/MapReduceZooExample.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Steps in MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/mapreduce_visual.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Assign tasks to each worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Map is another word for function: takes in data as one form, and \n",
    "transforms/maps it to another form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We create key-value pairs (tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Reorganize to make reducing easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Takes data from the map and _combines_ the data into smaller sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## MapReduce Examples with `mrjob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`mrjob` is a package that allows to write MapReduce jobs in Python which can be useful for testing on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Checkout the documentation for more information: https://mrjob.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can install it by running this cell (uncommented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # This package is for running MapReduce jobs with Python\n",
    "# ! pip install mrjob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Song Plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's what's in the `song_count.py` file:\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRCountSongs(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper),\n",
    "            MRStep(reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    # Each line in will be read as a key, value\n",
    "    # Each line has no key, so we ignore it with `_`\n",
    "    def mapper(self, _, song):\n",
    "        # Each line is a tuple: (song_names, 1) \n",
    "        yield (song, 1)\n",
    "\n",
    "    # Combine all tuples with the same key\n",
    "    def reducer(self, key, values):\n",
    "        # Key is the song name\n",
    "        # Sum up values in the tuple to get total song plays\n",
    "        yield (key, sum(values))\n",
    "        \n",
    "# Runs this if I call it via the terminal        \n",
    "if __name__ == \"__main__\":\n",
    "    MRCountSongs.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! head data/songplays.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! python ./src/song_count.py data/songplays.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# You can see more closely what's happening in the mapper step\n",
    "!python ./src/song_count.py data/songplays.txt --mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's what's in the `wordcount.py` file:\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "import string\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        # Remove punctuations\n",
    "        for s in string.punctuation:\n",
    "            line = line.replace(s, '')\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! python ./src/wordcount.py data/romeoandjuliet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
