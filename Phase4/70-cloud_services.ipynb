{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nerds.net/wp-content/uploads/2018/02/cloud-computer-reality-750x646.jpg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " - Explain the general concept of \"the cloud\";\n",
    " - Understand the cases where ***hardware acceleration*** is useful;\n",
    " - Understand the cases where ***cloud storage*** and the **Boto3** library in particular are useful;\n",
    " - Explain the purpose of ***deploying*** a machine learning model, particularly with the **Flask** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## What Is \"The Cloud\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For ***computationally intensive*** or ***long-running*** tasks, it doesn't make much sense to use a personal computer. Personal computers are not particularly powerful, and you also might want to turn them off or use their computing power to do other things.\n",
    "\n",
    "<a title=\"GNOME Project, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Gnome-computer.svg\"><img width=\"240\" alt=\"Gnome-computer\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Gnome-computer.svg/240px-Gnome-computer.svg.png\"></a>\n",
    "\n",
    "Before the cloud, organizations would typically have ***on-premises dedicated hardware*** for these tasks. This meant that they also needed IT systems administrators to manage the physical hardware.\n",
    "\n",
    "<a title=\"Akramusns, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Unique_Server_Racks.png\"><img width=\"512\" alt=\"Unique Server Racks\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Unique_Server_Racks.png/512px-Unique_Server_Racks.png\"></a>\n",
    "\n",
    "<small><i>Server racks used for web hosting (2014).</i></small>\n",
    "\n",
    "With cloud computing, ***hardware details are abstracted away*** and you can get on-demand computing power. The code is still running on a server maintained by the cloud provider, but you don't need an IT systems administrator to coordinate how the servers will be used.\n",
    "\n",
    "<a title=\"百楽兎, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Cloud_computing_icon.svg\"><img width=\"320\" alt=\"Cloud computing icon\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Cloud_computing_icon.svg/320px-Cloud_computing_icon.svg.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cloud Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Below is a graph showing some of the top cloud providers used by enterprise customers today:\n",
    "\n",
    "![public cloud adoption for enterprises](https://www.flexera.com/blog/wp-content/uploads/2021/03/Picture9.png)\n",
    "\n",
    "**Choosing a Cloud Provider**\n",
    "\n",
    "***On the job*** there will likely already be a preferred cloud provider that your employer uses, so you won't need to make a decision here. But ***as a student*** here are some things to consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Big-Name Providers\n",
    "\n",
    "Consider choosing to use one of the most popular providers, because this may help you in the job search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <img src=\"https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png\" width=350 alt=\"aws logo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**AWS** (Amazon Web Services) is currently the most popular cloud provider. In a previous Flatiron School analysis of the job market, we found that about 6% of entry-level Data Scientist roles specifically mentioned AWS as a required skill. AWS was the first true \"cloud services\" provider -- launching Simple Storage Service (S3) and Elastic Compute Cloud (EC2) in 2006, and is still very popular in part because they were the first of their kind. Check out this [Introduction to AWS for Data Scientists](https://www.dataquest.io/blog/introduction-to-aws-for-data-scientists/) for more information on navigating all of the available services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <a title=\"Microsoft Corporation, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Microsoft_Azure_Logo.svg\"><img width=\"320\" alt=\"Microsoft Azure Logo\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Microsoft_Azure_Logo.svg/320px-Microsoft_Azure_Logo.svg.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Microsoft Azure** (also just referred to as Azure) is the second most popular cloud provider. Our analysis found that about 2% of entry-level Data Scientist roles specifically mentioned Azure as a required skill. Azure launched later than AWS, and has very good compatibility with Windows tools and software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <img src=\"images/google-cloud-logo.png\" alt=\"google cloud logo\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Google Cloud** is the third most popular cloud provider. It did not appear in our entry-level Data Scientist role analysis as a requirement. It also launched later than AWS, and is compatible with other Google products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In general, more expensive services will perform better than cheaper services:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/630/1*Ao2QhhVEBEr2mJXL9jsuWQ.png\" alt=\"cost per hour vs. time to train\"/>\n",
    "<small><i>From <a href=\"https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a\" target=\"_blank\">Best Deals in Deep Learning Cloud Providers</a></i></small>\n",
    "\n",
    "As a student, you have no obligation to pay anything for cloud services! We are just letting you know that they exist, and what they can do for you.\n",
    "\n",
    "Some cloud services offer a 100% **free version** where you do not need to enter a credit card. These include:\n",
    "\n",
    "* [Google Colab](https://research.google.com/colaboratory/)\n",
    "* [Kaggle Kernels](https://www.kaggle.com/code)\n",
    "* [Databricks Community Edition](https://databricks.com/product/faq/community-edition)\n",
    "* [MongoDB Atlas](https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/)\n",
    "* [Heroku](https://www.heroku.com/)\n",
    "\n",
    "Other services will offer **free credits**. This includes AWS, Azure, Google Cloud, and others. They usually offer a default amount of free credit but will occasionally have special promotions for additional credit.\n",
    "\n",
    "To use free credits, you will typically need to enter credit card information, so make sure you pay attention to your free credit balance so you don't spend money that you don't intend to spend!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Why Would a Data Scientist Use Cloud Services?\n",
    "\n",
    "The two main reasons a data scientist would use cloud services are to ***get more computing power*** and to ***deploy machine learning models***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## More Computing Power\n",
    "\n",
    "Particularly with large datasets and tools like grid search that fit many different model iterations, training a model can take a **long time** on a personal computer. Maybe you have already had the experience of running a model and having to step away from the computer for minutes or even hours as the fan spins and the computer works hard to perform all of the necessary computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Hardware Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Nick Stathas, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Under_the_GPU.jpg\"><img width=\"256\" alt=\"Under the GPU\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Under_the_GPU.jpg/256px-Under_the_GPU.jpg\"></a>\n",
    "\n",
    "<small><i>A zoomed-in photo of the capacitors inside of a GPU.</i></small>\n",
    "\n",
    "As much as software libraries like NumPy or Spark can improve the efficiency of code, there is a limit to how much of a difference they can make, depending on the actual hardware of your computer.\n",
    "\n",
    "As a general concept, [hardware acceleration](https://www.omnisci.com/learn/resources/technical-glossary/hardware-acceleration) means using purpose-built hardware rather than general-purpose hardware.\n",
    "\n",
    "In the case of machine learning, this typically means running your code on a **GPU**, rather than a CPU.  A CPU _can_ do everything that a GPU can do, but it is not optimized for it, so it will likely take more time.  [This blog](https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a) argues that a CPU is to a GPU as a horse and buggy is to a car.\n",
    "\n",
    "One approach you might take would be to purchase a more powerful computer, with a GPU, more RAM, etc. and just use it for training models. But that can easily get very expensive!\n",
    "\n",
    "With a cloud service, you can train a machine learning model using GPU hardware, so the training should complete much more quickly than on a typical personal computer. And unlike having a dedicated computer, you're only paying for the computing power when you need it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cloud Instances/Containers with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Unknown authorUnknown author, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:SSH_diagram.png\"><img width=\"512\" alt=\"SSH diagram\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/c8/SSH_diagram.png\"></a>\n",
    "\n",
    "<small><i>SSH diagram</i></small>\n",
    "\n",
    "A cloud instance means you can run a customized, fully-fledged computer in the cloud. This often gives you the most fine-grained control but can also be much more expensive because they are not designed specifically for machine learning. Typically you will need to connect to a cloud instance via SSH, and you'll need to be comfortable navigating in a terminal interface.\n",
    "\n",
    "AWS Elastic Compute Cloud (EC2) is probably the most well-known cloud instance, and our analysis found that it was mentioned in about 2% of entry-level Data Scientist job postings.\n",
    "\n",
    "Here are some cloud container options with GPUs:\n",
    "\n",
    " - [AWS EC2](https://aws.amazon.com/blogs/machine-learning/train-deep-learning-models-on-gpus-using-amazon-ec2-spot-instances/)\n",
    " - [Google Cloud Platform](https://cloud.google.com/ml-engine/docs/using-gpus)\n",
    " - [IBM Watson Studio](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml_dlaas_gpus.html)\n",
    " - [Azure VM](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-gpu)\n",
    " - [Oracle Cloud](https://www.oracle.com/cloud/compute/gpu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cloud Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2019/12/01/Rhinestone-SageMaker-Studio-Page-2-v2.png\" width=500 alt=\"amazon sagemaker studio\"/>\n",
    "\n",
    "<small><i>From <b>Amazon SageMaker Studio: The First Fully Integrated Development Environment For Machine Learning</b> on the <a href=\"https://aws.amazon.com/blogs/aws/amazon-sagemaker-studio-the-first-fully-integrated-development-environment-for-machine-learning/\" target=\"blank_\">AWS News Blog</a></i></small>\n",
    "\n",
    "Compared to virtual machines, cloud notebooks tend to be easier to work with because they allow you to use the familiar notebook interface. Some of them even have free GPUs or TPUs! Even without hardware acceleration, cloud notebooks can allow you to train models in the cloud and free up resources on your personal computer to do other tasks.\n",
    "\n",
    "Here are some cloud notebooks to consider:\n",
    "\n",
    " - [AWS Sagemaker](https://aws.amazon.com/machine-learning/accelerate-machine-learning-P3/)\n",
    " - [Databricks Community Edition](https://community.cloud.databricks.com/)\n",
    " - [Google Colab](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c)\n",
    " - [Kaggle kernels](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu)\n",
    " - [data.world](https://jupyter.data.world/)\n",
    " \n",
    "Because there is a GPU available in the free tier, Google Colab is the most popular of these tools for our students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Kottakkalnet, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Plastic_bucket_IMG_20160701_161628956.jpg\"><img width=\"512\" alt=\"Plastic bucket IMG 20160701 161628956\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Plastic_bucket_IMG_20160701_161628956.jpg/512px-Plastic_bucket_IMG_20160701_161628956.jpg\"></a>\n",
    "\n",
    "It's annoying to have huge data files taking up space on your laptop, and if you want to train your model in the cloud, your data also needs to be in the cloud.  But for reasons related to hardware acceleration, it can get pretty expensive to store large datasets in general-purpose cloud services like an EC2 instance or a cloud VM.  That's when cloud storage services become useful.\n",
    "\n",
    "#### Cloud Storage Buckets\n",
    "\n",
    "The major providers of storage \"buckets\" are:\n",
    "\n",
    " - [AWS S3](https://aws.amazon.com/s3/getting-started/)\n",
    " - [Google Cloud Storage](https://cloud.google.com/storage/)\n",
    " - [Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-introduction)\n",
    "\n",
    "These tools are designed for uploads of raw files, e.g. folders full of images, CSVs, or JSONs.\n",
    "\n",
    "They each cost about 2-5 cents per GB per month.  AWS S3 is the oldest and tends to have the most integration support with other platforms, although you may need to use Google storage if you're using other Google products or Azure storage if you're using other Azure products.\n",
    "\n",
    "**Boto3** is the Python library used to connect to S3, and there is a demonstration of how to use it in the \"Level Up\" portion of this notebook.\n",
    "\n",
    "#### Cloud Databases\n",
    "\n",
    "If you want to deploy a website where new information gets saved (what kinds of queries users perform, user ratings of the quality of predictions, etc.) then you need a cloud database.  These work roughly the same as a database running on your computer.\n",
    "\n",
    "Using a cloud database is mainly an opportunity to practice using tooling that you are likely to use on the job, because they assist with collaboration.\n",
    "\n",
    "Some popular providers are:\n",
    "\n",
    " - [Heroku Postgres](https://www.heroku.com/postgres)\n",
    " - [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)\n",
    " - [AWS Aurora](https://aws.amazon.com/rds/aurora/)\n",
    " - [AWS RDS](https://aws.amazon.com/rds/)\n",
    "\n",
    "Most of these tools have a free tier, which permits a limited number of records to be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deploying ML Models\n",
    "\n",
    "Typically in this program we have used Jupyter Notebooks to build, train, and evaluate models. Jupyter Notebook is a very useful interface, but a predictive model that only exists in the context of a notebook is not particularly useful in the real world!\n",
    "\n",
    "Some key tools and techniques to be aware of for deploying ML models include model persistence (pickling), deploying as an API, and deploying as a full-stack web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Renee Comet (photographer), Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Pickle.jpg\"><img width=\"256\" alt=\"Pickle\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Pickle.jpg/256px-Pickle.jpg\"></a>\n",
    "\n",
    "Recall that there is a difference between a file ***on disk*** and a variable ***in memory***. Variables in memory only persist until the notebook kernel is shut down, whereas files on disk persist as long as there is functional storage hardware.\n",
    "\n",
    "When you first train a model, it only exists in memory. The process for storing it on disk is called ***pickling***. This is a type of serialization where the trained model gets stored in a file, conventionally using a `.pkl` extension. There is an example in the \"Level Up\" section of this notebook demonstrating the fitting, pickling, and un-pickling of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Deploying a Model as an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/request_response_cycle.png\" width=600 alt=\"client-server model and request-response cycle\"/>\n",
    "\n",
    "<small><i>Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" target=\"blank_\">Freepik</a> from www.flaticon.com</i></small>\n",
    "\n",
    "Once you have a pickled model, in theory anyone who can execute Python code can then un-pickle the model and use it to make predictions.\n",
    "\n",
    "However, what if you want someone to be able to use your model even if they aren't running Python code? For example, what if the model is being used in the context of a website or a mobile app?\n",
    "\n",
    "In that case, developing an HTTP ***API*** can be useful because this protocol is compatible with many different programming languages.\n",
    "\n",
    "An example interaction with a model deployed as an API would be:\n",
    "\n",
    "1. Client request: `POST /predict '{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}'`\n",
    "2. Server response: `'{\"predicted_class\": 0}'`\n",
    "\n",
    "Just like there are multiple tools that can make a `POST` request (Python `requests` library, `cURL` in the terminal, JavaScript `fetch` in a web appication), there are multiple ways you can build your server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Deploying a Model as an API with Cloud Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Wvbailey, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Function_machine2.svg\"><img width=\"243\" alt=\"Function machine2\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Function_machine2.svg/243px-Function_machine2.svg.png\"></a>\n",
    "\n",
    "The most minimal way to deploy a machine learning model is to create a ***cloud function***. This means that you created a pickled model, write a few lines of Python code specifying how to un-pickle the model and use it to make predictions, and deploy it with a cloud service designed for this purpose.\n",
    "\n",
    "[This curriculum lesson](https://github.com/learn-co-curriculum/dsc-pickling-pipelines) walks through the process of developing and deploying a Google Cloud function, including the process of pickling a full pipeline. The complete Google Cloud function from that lesson looks like this:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "def iris_prediction(sepal_length, sepal_width, petal_length, petal_width):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = joblib.load(f)\n",
    "    X = [[sepal_length, sepal_width, petal_length, petal_width]]\n",
    "    predictions = model.predict(X)\n",
    "    prediction = int(predictions[0])\n",
    "    return {\"predicted_class\": prediction}\n",
    "\n",
    "def predict(request):\n",
    "    request_json = request.get_json()\n",
    "    result = iris_prediction(**request_json)\n",
    "    return json.dumps(result)\n",
    "```\n",
    "\n",
    "While they involve writing the least amount of code of any model deployment option, cloud functions can be tricky to configure within the cloud service. Looking at the code above you might notice that the `predict` function is never actually invoked in the code -- when you configure the cloud function, you have to specify that this function should be called. You will also need to configure the cloud function so that it can accept public web requests, and typically you won't be able to test anything on your local computer, so this can be a slow back-and-forth of tweaking the configuration until it works.\n",
    "\n",
    "We found that the Google Cloud functions were the easiest to work with, but you also might want to check out [AWS Lambda Functions](https://docs.aws.amazon.com/lambda/latest/dg/python-programming-model.html) and [Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-first-function-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Deploying a Model as an API with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Armin Ronacher, Copyrighted free use, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Flask_logo.svg\"><img width=\"256\" alt=\"Flask logo\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Flask_logo.svg/256px-Flask_logo.svg.png\"></a>\n",
    "\n",
    "***Flask*** is a [microframework](https://flask.palletsprojects.com/en/2.0.x/foreword/#what-does-micro-mean) for web development with Python. It can be used for full-stack web applications, but it's also very popular for deploying machine learning models as APIs. In fact, the Google Cloud function Python tooling that is used in the curriculum lesson linked above uses Flask \"under the hood\"!\n",
    "\n",
    "Here is that same cloud function, rewritten as a Flask app:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def iris_prediction(sepal_length, sepal_width, petal_length, petal_width):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = joblib.load(f)\n",
    "    X = [[sepal_length, sepal_width, petal_length, petal_width]]\n",
    "    predictions = model.predict(X)\n",
    "    prediction = int(predictions[0])\n",
    "    return {\"predicted_class\": prediction}\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return 'Hello, world!'\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    request_json = request.get_json()\n",
    "    result = iris_prediction(**request_json)\n",
    "    return json.dumps(result)\n",
    "```\n",
    "\n",
    "It's definitely a bit longer than the cloud function version, but it has two benefits:\n",
    "\n",
    "1. You can actually run the server on your local computer, which makes **debugging** much faster and easier.\n",
    "2. Rather than using Google Cloud functions (a paid service, although there are free credits when you sign up), you can use **Heroku** to deploy your API, which is **100% free** for up to 5 apps. (You can also deploy using an EC2 instance or other cloud container if your model is too large/slow for Heroku.)\n",
    "\n",
    "Check out these curriculum lessons for more info: [Introduction to Flask](https://github.com/learn-co-curriculum/dsc-flask-intro), [Deploying a Model with Flask](https://github.com/learn-co-curriculum/dsc-flask-deployment).\n",
    "\n",
    "Besides Heroku and AWS EC2, you can also host flask apps with [Azure App Service](https://docs.microsoft.com/en-us/learn/modules/host-a-web-app-with-azure-app-service/index), [DigitalOcean](https://www.digitalocean.com/community/tutorials/how-to-deploy-a-flask-application-on-an-ubuntu-vps), [Google Cloud App Engine](https://cloud.google.com/python/getting-started/hello-world), and [AWS Elatic Beanstalk](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Deploying a Model as a Full-Stack Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An API interface is very useful, but you also might want something more interactive and impressive for your data science portfolio. Developing a ***full-stack web app*** means that there is a web page interface, so once your app is deployed, someone can load it directly in the browser and generate model predictions by clicking on the page.\n",
    "\n",
    "#### Deploying a Model as a Full-Stack Web App with Flask\n",
    "\n",
    "<img src=\"images/model_view_controller.png\" width=600 alt=\"model view controller diagram\" />\n",
    "<small><i>Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" target=\"blank_\">Freepik</a> from www.flaticon.com</i></small>\n",
    "\n",
    "The same microframework described for deploying a model as an API can also be used to make a full-stack web app. This requires that you write HTML and CSS (and optionally JavaScript) as well as Python in order to create the \"view\" component of the model-view-controller (MVC) framework.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* If you already have experience with HTML and CSS, this approach can allow you to flex your creativity and make something very polished that shows off all of your skills. You can add as many pages as you want (e.g. to create a portfolio website rather than a single-page application)\n",
    "* You can generate data visualizations in Python with Matplotlib (don't need to learn any new plotting libraries)\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* If you don't have any experience with HTML and CSS, the learning curve can be steep. These languages don't produce straightforward error messages like Python does, so it can be quite difficult to know where your mistakes are\n",
    "* Unless you know JavaScript and are prepared to work with a library like [D3.js](https://d3js.org/), your visualizations aren't going to be interactive\n",
    "\n",
    "If you are interested in using this approach, check out this [template repository](https://github.com/learn-co-students/capstone-flask-app-template-082420), which includes instructions in the README for modifying the HTML and Python code so that it will work for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Deploying a Model as a Full-Stack Web App with Dash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/dash_app.png\" />\n",
    "\n",
    "***Plotly Dash*** (also just referred to as Dash) is \"the most downloaded, trusted framework for building machine learning web apps in Python\" ([source](https://plotly.com/building-machine-learning-web-apps-in-python/)). It allows you to create interactive websites that make predictions from machine learning models, all without writing HTML, CSS, or JavaScript directly.\n",
    "\n",
    "Dash is built on top of Flask, so it can be deployed using Heroku as well. Check out [this link](https://calm-bastion-07515.herokuapp.com/) for an example app hosted on Heroku (might take a while to load if it hasn't been used in a while), and these curriculum lessons for more info: [Introduction to Dash](https://github.com/learn-co-curriculum/dsc-dash-intro), [Deploying a Model with Dash](https://github.com/learn-co-curriculum/dsc-dash-deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Cloud services are useful for **computationally intensive or long-running tasks**\n",
    "* The major providers of cloud services are **Amazon Web Services (AWS), Microsoft Azure, and Google Cloud**\n",
    "* As a data scientist, you will generally use cloud services to **get more computing power and/or to deploy machine learning models**\n",
    "* If you want to get more computing power, consider:\n",
    "  * Cloud instances/containers with GPUs, particularly **EC2**\n",
    "  * Cloud notebooks, particularly **Google Colab**\n",
    "  * Cloud storage, particularly S3 bucket storage with **Boto3**\n",
    "* If you want to deploy a machine learning model, first pickle the model, then consider:\n",
    "  * Deploying a model as an API, using either a **cloud function** or a minimal **Flask** app\n",
    "  * Deploying a model as a full-stack web app, either using Flask or **Dash**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Up: Cloud Notebook Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's upload the same notebook to a couple different platforms, as an illustration.\n",
    "\n",
    " - [AWS Sagemaker](https://aws.amazon.com/machine-learning/accelerate-machine-learning-P3/)\n",
    " - [Databricks Community Edition](https://community.cloud.databricks.com/)\n",
    " - [Google Colab](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c)\n",
    " - [Kaggle kernels](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu)\n",
    " - [data.world](https://jupyter.data.world/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Level Up: Pickling a Model for Deployment Demo\n",
    "\n",
    "This shows the basic outline for training a model, evaluating it, then using it in a \"production\" context to make predictions about new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "We'll use the wine dataset from scikit-learn for a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scikit-learn imports\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# get premade wine dataset from sklearn\n",
    "data = load_wine()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# let's build a model to predict the class of wine\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=100)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the model accuracy\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# display the confusion matrix\n",
    "metrics.confusion_matrix(y_test, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pickling the Model\n",
    "\n",
    "The [`pickle` format](https://docs.python.org/3/library/pickle.html) is built into the Python language. It's called pickling because it is a form of preserving an object in memory for later. This is achieved by converting everything about the Python variable into bits of data that can be stored in a file.\n",
    "\n",
    "For scikit-learn models, [the `joblib` library is recommended instead](https://scikit-learn.org/stable/modules/model_persistence.html). This works similarly to `pickle` but has built-in functionality that works better with NumPy arrays.\n",
    "\n",
    "In the cell below, we take our `classifier` variable and turn it into a file on disk called `wine_classifier.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# use the built-in open() function to open a file\n",
    "output_file = open(\"wine_classifier.pkl\", \"wb\") # \"wb\" means \"write as bytes\"\n",
    "# dump the variable's contents into the file\n",
    "joblib.dump(classifier, output_file)\n",
    "# close the file, ensuring nothing stays in the buffer\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Loading the Model\n",
    "\n",
    "This part would actually almost never be in the same file as the previous step. The goal is to take information that was stored in memory at one time, then save it so it can be used later. Here specifically this is useful because training a model is usually a lot slower than using the model to make a prediction, so this saves us from having to re-run that costly operation each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use the built-in open() function again, this time to read\n",
    "model_file = open(\"wine_classifier.pkl\", \"rb\") # \"rb\" means \"read as bytes\"\n",
    "# load the variable's contents from the file into a variable\n",
    "loaded_model = joblib.load(model_file)\n",
    "# close the file\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Making a Prediction with the Loaded Model\n",
    "\n",
    "In this section I'm constructing a request JSON that resembles what would come from a user who wants a predicted class of wine based on these feature values. This code would not actually exist in your deployed application, it would be created automatically by whatever protocol generated the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make a fake request JSON from the user with all the headings\n",
    "\n",
    "request_json = {}\n",
    "\n",
    "expected_features = (\"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\", \\\n",
    "        \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \\\n",
    "        \"Proanthocyanins\", \"Color intensity\", \"Hue\", \\\n",
    "        \"OD280/OD315 of diluted wines\", \"Proline\")\n",
    "example_values = [1.282e+01, 3.370e+00, 2.300e+00, 1.950e+01, 8.800e+01, 1.480e+00, \\\n",
    "       6.600e-01, 4.000e-01, 9.700e-01, 1.026e+01, 7.200e-01, 1.750e+00, \\\n",
    "       6.850e+02]\n",
    "\n",
    "for i, feature in enumerate(expected_features):\n",
    "    request_json[feature] = example_values[i]\n",
    "request_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the section that more closely resembles what you might have in your application. I'm checking to make sure that the expected values are in the request_json, transforming them into the right format to make a prediction, then printing out that prediction. In your actual deployed code, you would most likely be **returning** the response, not printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if request_json and all(feature in request_json for feature in expected_features):\n",
    "    \n",
    "    # unpack all of the relevant values from the request into a list\n",
    "    \n",
    "    test_value = [request_json[feature] for feature in expected_features]\n",
    "    \n",
    "    # make a prediction from the \"user input\"\n",
    "    \n",
    "    predicted_class = int(loaded_model.predict([test_value])[0])\n",
    "    \n",
    "    # construct a response\n",
    "    \n",
    "    response_json = {\"prediction\": predicted_class}\n",
    "    print(response_json)\n",
    "else:\n",
    "    print(\"something was missing from the request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a more extended explanation of pickling, check out these curriculum lessons: [Pickle](https://github.com/learn-co-curriculum/dsc-pickle) and [Pickling and Deploying Pipelines](https://github.com/learn-co-curriculum/dsc-pickling-pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Level Up: AWS S3 Buckets with Boto3 Demo\n",
    "\n",
    "For the purpose of this example, the `wine_classifier.pkl` file has already been uploaded to the Flatiron School Curriculum AWS account. To make your own pickle file available from code, you would need to make an account and upload the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Optional Prerequisite: CLI Interface\n",
    "\n",
    "Installation instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html), CLI docs [here](https://docs.aws.amazon.com/cli/latest/reference/s3/).  You will need to use this to upload large files (somewhere around 160 MB) but it's clunkier than integrating directly into Python and won't work with all deployment techniques.\n",
    "\n",
    "If you want to upload a document using the CLI interface, that will look something like this:\n",
    "\n",
    "```bash\n",
    "aws s3 cp s3://<your bucket name>/wine_classifier.pkl wine_classifier.pkl\n",
    "```\n",
    "\n",
    "### Using Boto3 to Retrieve a Pickled Model\n",
    "\n",
    "![AWS CLI plus Python equals Boto3](https://curriculum-content.s3.amazonaws.com/data-science/images/boto3.png)\n",
    "\n",
    "Boto 3 is a library that allows Python developers to access many different Amazon web services, not just S3. You can find the full list [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/index.html). But we'll focus on using S3 with boto3 because that is one of the most common use cases for data scientists.\n",
    "\n",
    "If you are accessing public resources in an S3 bucket, the interface is pretty simple. For example, here is how you would load the pickled wine classifier from an S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# instantiate a connection to the S3 resource\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "# load an object from that resource\n",
    "pkl_obj = s3.Object(bucket_name=\"curriculum-content\", key=\"data-science/wine_classifier.pkl\")\n",
    "\n",
    "# get the response (under the hood this is a similar to `requests`)\n",
    "pkl_resp = pkl_obj.get()[\"Body\"].read()\n",
    "\n",
    "# look at what's in there (first 100 characters)\n",
    "pkl_resp[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Loading a Pickled Model from Boto3\n",
    "\n",
    "As you can see from the print-out above, we have a `RandomForestClassifier`. But right now it's still encoded as a string of bytes rather than loaded into a scikit-learn model.\n",
    "\n",
    "To load it into an actual model, we'll need to use `BytesIO` (a class in the built-in [Python `io` module](https://docs.python.org/3/library/io.html)) and then we'll be able to load it with `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# adding this so we'll ignore warnings about our scikit-learn version being different\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "# read the string of bytes into BytesIO object\n",
    "pkl_bytes = BytesIO(pkl_resp)\n",
    "\n",
    "# load the model using joblib\n",
    "boto3_loaded_model = joblib.load(pkl_bytes)\n",
    "boto3_loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# display model params\n",
    "boto3_loaded_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using Boto3 to Retrieve a CSV\n",
    "\n",
    "The process for loading a CSV is essentially the same as loading a pickled model, except that you can pass the `BytesIO` object directly to `pandas`, rather than needing `joblib` to perform an additional loading step first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load another object (same bucket name, different key this time)\n",
    "csv_obj = s3.Object(bucket_name=\"curriculum-content\", key=\"data-science/data/wine.csv\")\n",
    "\n",
    "# get the response\n",
    "csv_resp = csv_obj.get()[\"Body\"].read()\n",
    "\n",
    "# look at what's in there (first 100 characters)\n",
    "csv_resp[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So far much more readable than the pickled model! This is because it had a different encoding setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the string of bytes into BytesIO object\n",
    "csv_bytes = BytesIO(csv_resp)\n",
    "\n",
    "# read the csv file into a dataframe using pandas\n",
    "boto3_loaded_df = pd.read_csv(csv_bytes)\n",
    "boto3_loaded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Optional Library: S3Fs\n",
    "\n",
    "If you are regularly loading data files from S3 using `pandas`, you might want to look into S3Fs. This is a tool that allows you to pass an S3 path directly into a `pandas` function like `pd.read_csv`.\n",
    "\n",
    "First you'll need to install `s3fs` (docs [here](https://s3fs.readthedocs.io/en/latest/)) which allows Python to access S3 buckets like they are part of the local file system. You just start the file path with `s3://`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !conda install s3fs -c conda-forge -y\n",
    "from os import path\n",
    "file_path = path.join(\"s3://\", \"curriculum-content\", \"data-science/data/wine.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
