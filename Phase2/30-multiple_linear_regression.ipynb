{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Multiple-Linear-Regression\" data-toc-modified-id=\"Multiple-Linear-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Multiple Linear Regression</a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Regression-with-Multiple-Predictors\" data-toc-modified-id=\"Regression-with-Multiple-Predictors-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regression with Multiple Predictors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Expanding-Simple-Linear-Regression\" data-toc-modified-id=\"Expanding-Simple-Linear-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Expanding Simple Linear Regression</a></span></li><li><span><a href=\"#Closed-form-Solution\" data-toc-modified-id=\"Closed-form-Solution-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Closed-form Solution</a></span></li></ul></li><li><span><a href=\"#Confounding-Variables\" data-toc-modified-id=\"Confounding-Variables-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Confounding Variables</a></span></li><li><span><a href=\"#Dealing-with-Categorical-Variables\" data-toc-modified-id=\"Dealing-with-Categorical-Variables-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dealing with Categorical Variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dummying\" data-toc-modified-id=\"Dummying-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Dummying</a></span></li></ul></li><li><span><a href=\"#Multiple-Regression-in-statsmodels\" data-toc-modified-id=\"Multiple-Regression-in-statsmodels-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Multiple Regression in <code>statsmodels</code></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Diamonds-Dataset\" data-toc-modified-id=\"Diamonds-Dataset-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Diamonds Dataset</a></span></li><li><span><a href=\"#Check-distribution-of-target\" data-toc-modified-id=\"Check-distribution-of-target-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Check distribution of target</a></span></li><li><span><a href=\"#Build-model-with-log-scaled-target\" data-toc-modified-id=\"Build-model-with-log-scaled-target-6.0.3\"><span class=\"toc-item-num\">6.0.3&nbsp;&nbsp;</span>Build model with log-scaled target</a></span></li></ul></li></ul></li><li><span><a href=\"#Putting-it-in-Practice:-Wine-Dataset-üç∑\" data-toc-modified-id=\"Putting-it-in-Practice:-Wine-Dataset-üç∑-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Putting it in Practice: Wine Dataset üç∑</a></span><ul class=\"toc-item\"><li><span><a href=\"#üß†-Knowledge-Check\" data-toc-modified-id=\"üß†-Knowledge-Check-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>üß† <strong>Knowledge Check</strong></a></span></li><li><span><a href=\"#Running-the-Regression\" data-toc-modified-id=\"Running-the-Regression-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Running the Regression</a></span></li></ul></li><li><span><a href=\"#Scaling---The-Missing-&amp;-Helpful-Step\" data-toc-modified-id=\"Scaling---The-Missing-&amp;-Helpful-Step-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Scaling - The Missing &amp; Helpful Step</a></span><ul class=\"toc-item\"><li><span><a href=\"#What's-Going-on-Here?\" data-toc-modified-id=\"What's-Going-on-Here?-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>What's Going on Here?</a></span></li><li><span><a href=\"#A-Solution:-Standard-Scaling\" data-toc-modified-id=\"A-Solution:-Standard-Scaling-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>A Solution: Standard Scaling</a></span></li><li><span><a href=\"#Redoing-with-Standard-Scaling\" data-toc-modified-id=\"Redoing-with-Standard-Scaling-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Redoing with Standard Scaling</a></span></li><li><span><a href=\"#üß†-Knowledge-Check\" data-toc-modified-id=\"üß†-Knowledge-Check-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>üß† <strong>Knowledge Check</strong></a></span></li><li><span><a href=\"#üß†-Knowledge-Check\" data-toc-modified-id=\"üß†-Knowledge-Check-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>üß† <strong>Knowledge Check</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Follow-Up\" data-toc-modified-id=\"Follow-Up-8.5.1\"><span class=\"toc-item-num\">8.5.1&nbsp;&nbsp;</span>Follow-Up</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-Regression-in-Scikit-Learn\" data-toc-modified-id=\"Multiple-Regression-in-Scikit-Learn-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Multiple Regression in Scikit-Learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scale-the-Data\" data-toc-modified-id=\"Scale-the-Data-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Scale the Data</a></span></li><li><span><a href=\"#Fit-the-Model\" data-toc-modified-id=\"Fit-the-Model-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Fit the Model</a></span></li><li><span><a href=\"#Evaluate-Performance\" data-toc-modified-id=\"Evaluate-Performance-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Evaluate Performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Observing-Residuals\" data-toc-modified-id=\"Observing-Residuals-9.3.1\"><span class=\"toc-item-num\">9.3.1&nbsp;&nbsp;</span>Observing Residuals</a></span></li><li><span><a href=\"#Sklearn-Metrics\" data-toc-modified-id=\"Sklearn-Metrics-9.3.2\"><span class=\"toc-item-num\">9.3.2&nbsp;&nbsp;</span>Sklearn Metrics</a></span></li></ul></li></ul></li><li><span><a href=\"#Level-Up:-Deeper-Evaluation-of-Wine-Data-Predictions\" data-toc-modified-id=\"Level-Up:-Deeper-Evaluation-of-Wine-Data-Predictions-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Level Up: Deeper Evaluation of Wine Data Predictions</a></span></li><li><span><a href=\"#Level-Up:-Regression-with-Categorical-Features-with-the-Comma-Dataset\" data-toc-modified-id=\"Level-Up:-Regression-with-Categorical-Features-with-the-Comma-Dataset-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Level Up: Regression with Categorical Features with the Comma Dataset</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as metrics\n",
    "from random import gauss\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mlr](https://miro.medium.com/max/1280/1*lJKFo3yyZaFIx4ET1dLmlg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the one-hot strategy to encode categorical variables\n",
    "- Conduct linear regressions in `statsmodels`\n",
    "- Use standard scaling for linear regression for better  interpretation\n",
    "- Conduct linear regressions in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Multiple Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's all a bunch of dials\n",
    "\n",
    "<img width='450px' src='./images/dials.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea here is pretty simple. Whereas, in simple linear regression we took our dependent variable to be a function only of a single independent variable, here we'll be taking the dependent variable to be a function of multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regression equation, then, instead of looking like $\\hat{y} = mx + b$, will now look like:\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + ... + \\hat{\\beta}_nx_n$.\n",
    "\n",
    "Remember that the hats ( $\\hat{}$ ) indicate parameters that are estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this still a best-fit *line*? Well, no. What does the graph of, say, z = x + y look like? [Here's](https://academo.org/demos/3d-surface-plotter/) a 3d-plotter. (Of course, once we get x's with subscripts beyond 2 it's going to be very hard to visualize. But in practice linear regressions can make use of dozens or even of hundreds of independent variables!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-form Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to calculate the betas by hand? Yes, a multiple regression problem still has a closed-form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a word, for a multiple linear regression problem where $X$ is the matrix of independent variable values and $y$ is the vector of dependent variable values, the vector of optimizing regression coefficients $\\vec{b}$ is given by:\n",
    "\n",
    "$\\vec{b} = (X^TX)^{-1}X^Ty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus more directly on matrix mathematics later in the course, so don't worry if this equation is opaque to you. See [here](https://stattrek.com/multiple-regression/regression-coefficients.aspx) for a nice explanation and example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confounding Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I have a simple linear regression that models the growth of corn plants as a function of the temperature of the ambient air. And suppose there is a noticeable positive correlation between temperature and plant height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corn = pd.read_csv('data/corn.csv',\n",
    "                  usecols=['temp', 'humid', 'height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=corn, x='temp', y='height')\n",
    "plt.xlabel('Temperature ($\\degree$ F)')\n",
    "plt.ylabel('Height (cm)')\n",
    "plt.title('Corn plant height as a function of temperature');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that higher temperatures lead to taller corn plants. But it's hard to know for sure. One **confounding variable** might be *humidity*. If we haven't controlled for humidity, then it's difficult to draw conclusions.\n",
    "\n",
    "One solution is to use **both features** in a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=corn, x='humid', y='height')\n",
    "plt.xlabel('Humidity (%)')\n",
    "plt.ylabel('Height (cm)')\n",
    "plt.title('Corn plant height as a function of humidity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 6)).add_subplot(111, projection='3d')\n",
    "ax.scatter(corn['temp'], corn['humid'], corn['height'],\n",
    "           depthshade=True, s=40, color='#ff0000')\n",
    "# create x,y\n",
    "xx, yy = np.meshgrid(corn['temp'], corn['humid'])\n",
    "\n",
    "# calculate corresponding z\n",
    "z = 4.3825 * xx + 2.4693 * yy - 255.5434\n",
    "\n",
    "# plot the surface\n",
    "ax.plot_surface(xx, yy, z, alpha=0.01, color='#00ff00')\n",
    "\n",
    "ax.view_init(30, azim=240)\n",
    "ax.set_xlabel('Temperature ($\\degree$ F)')\n",
    "ax.set_ylabel('Humidity (%)')\n",
    "ax.set_zlabel('Height (cm)')\n",
    "plt.title('Corn plant height as a function of temperature and humidity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One risk we run when adding more predictors to a model is that their correlations with the target may be nearly *collinear* with each other. This can make it difficult to determine which predictor is doing the heavy lifting. We shall explore this theme of **multicollinearity** in more depth in due course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue we'd like to resolve is what to do with categorical variables, i.e. variables that represent categories rather than continua. In a Pandas DataFrame, these columns may well have strings or objects for values, but they need not. A certain heart-disease dataset from Kaggle, for example, has a target variable that takes values 0-4, each representing a different stage of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very effective way of dealing with categorical variables is to dummy them out. What this involves is making a new column for _each categorical value in the column we're dummying out_.\n",
    "\n",
    "These new columns will be filled only with 0's and 1's, a 1 representing the presence of the relevant categorical value.\n",
    "\n",
    "Let's look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use = pd.read_csv('data/comma-survey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this dataset see [here](https://fivethirtyeight.com/features/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use['In your opinion, which sentence is more gramatically correct?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comma_use.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try using sklearn's OneHotEncoder to create our dummy columns:\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "comma_trans = ohe.fit_transform(comma_use.drop('RespondentID', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we have used ```pd.get_dummies()``` instead?\n",
    "\n",
    "Well, yes. And in fact ```get_dummies()``` is in some ways easier; for one thing, it's built right into Pandas. But there are drawbacks with it as well. The main advantage of the `sklearn` tool is that it stores information about the columns and creates a persistent function that can be used on future data of the same form. See [this page](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(comma_use.drop('RespondentID', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did the encoder do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_trans.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_df = pd.DataFrame(comma_trans.todense(), columns=ohe.get_feature_names())\n",
    "comma_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression in `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a multiple regression with `statsmodels`. Let's start with a toy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.arange(1, 6)\n",
    "preds = np.array([stats.norm(loc=center, scale=3).rvs(200) for center in centers]).T\n",
    "\n",
    "preds_df = pd.DataFrame(preds, columns=[f'var{center}' for center in centers])\n",
    "\n",
    "target = preds_df['var1'] + 2*preds_df['var2'] + 3*preds_df['var3']\\\n",
    "    + 4*preds_df['var4'] + 5*preds_df['var5']\n",
    "target_df = pd.DataFrame(target, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([preds_df, target_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(endog=y, exog=X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diamonds Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds').drop(['cut', 'color', 'clarity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop('price', axis=1), data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = sm.OLS(y, X).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.graphics.plot_regress_exog(model2, 'carat', fig=plt.figure(figsize=(12, 8)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check distribution of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = int( len(y)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=y, bins=bins, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=y_scld, bins=bins, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model with log-scaled target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = sm.OLS(y_scld, X).fit()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.plot_regress_exog(model3, 'carat', fig=plt.figure(figsize=(12, 8)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that $R^2$ can be negative!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pred = np.mean(y) * np.ones(len(y))\n",
    "worse_pred = (np.mean(y) + 1000) * np.ones(len(y))\n",
    "\n",
    "print(metrics.r2_score(y, bad_pred))\n",
    "print(metrics.r2_score(y, worse_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it in Practice: Wine Dataset üç∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset includes measurable attributes of different wines as well as their rated quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('data/wine.csv')\n",
    "\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we want to attempt to estimate the perceived quality of a wine using these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['red_wine'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why are we using \"quality\" as the dependent variable (target)? Would it make sense for another feature to be the target instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll separate the data into our predictors (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_preds = wine.drop('quality', axis=1)\n",
    "wine_target = wine['quality']\n",
    "wine_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform our (multiple) linear regression! Since we already used `statsmodels`, let's use that again to fit the model and then check the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sm.add_constant() to add constant term/y-intercept\n",
    "predictors = sm.add_constant(wine_preds)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(wine_target, predictors).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All right! So we fitted our model! Take a look at the summary and look if you can understand the different parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling - The Missing & Helpful Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you looked at the summary after we did the linear regression, you might have noticed something interesting.\n",
    "\n",
    "Observing the coefficients, you might notice there are two relatively large coefficients and nearly rest are less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Going on Here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a word, it's useful to have all of our variables be on the same scale, so that the resulting coefficients are easier to interpret. If the scales of the variables are very different one from another, then some of the coefficients may end up on very large or very tiny scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens since the coefficients will effectively attempt to \"shrink\" or \"expand\" the features before factoring their importance to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/shrinkinator.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can make it more difficult for interpretation and identifying coefficients with the most \"effect\" on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this, see [this post](https://stats.stackexchange.com/questions/32649/some-of-my-predictors-are-on-very-different-scales-do-i-need-to-transform-them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Solution: Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to *scale* our features. There are a few ways to do this but we'll focus on **standard scaling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do **standard scaling**, we're really scaling it to be the features' respective $z$-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits:\n",
    "\n",
    "- This tends to make values relatively small (mean value is at $0$ and one standard deviation $\\sigma$ from the mean is $1$).\n",
    "- Easier interpretation: larger coefficients tend to be more influential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time, let's *scale* our columns as $z$-scores first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Redoing with Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try standard scaling the model with our wine dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll include all the columns for now.\n",
    "\n",
    "wine_preds_scaled = (wine_preds - np.mean(wine_preds)) / np.std(wine_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_preds_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictors = sm.add_constant(wine_preds_scaled)\n",
    "model = sm.OLS(wine_target, predictors).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check how well this model did with the one before scaling. Does it perform any differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After standard scaling, what would it mean when all the $x_i$ are all $0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does this mean for the constant term $\\hat{\\beta}_0$? Could we check this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_target.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's great that we tried out multiple linear regression with `statsmodels`; now let's try it with `sklearn`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a StandardScaler object to scale our data for us.\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll apply it to our data by using the .fit() and .transform() methods.\n",
    "ss.fit(wine_preds)\n",
    "\n",
    "wine_preds_st_scaled = ss.transform(wine_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_preds_st_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the scaling worked about the same as when we did it by hand\n",
    "np.allclose(wine_preds_st_scaled, wine_preds_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_preds_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_preds_st_scaled[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit a `LinearRegression` object to our training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit a LinearRegression object to our training data!\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(wine_preds_st_scaled, wine_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the .coef_ attribute to recover the results\n",
    "# of the regression.\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(wine_preds_st_scaled, wine_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lr.predict(wine_preds_st_scaled)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left is to evaluate our model to see how well it did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the residuals like we would for a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lr.predict(wine_preds_st_scaled)\n",
    "resid = (wine_target - y_hat)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=range(y_hat.shape[0]),y=resid, alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics module in sklearn has a number of metrics that we can use to measure the accuracy of our model, including the $R^2$ score, the mean absolute error and the mean squared error. Note that the default 'score' on our model object is the $R^2$ score. Let's go back to our wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this metric is properly calibrated. If we put simply $\\bar{y}$ as our prediction, then we should get an $R^2$ score of *0*. And if we predict, say, $\\bar{y} + 1$, then we should get a *negative* $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_quality = np.mean(wine_target)\n",
    "num = len(wine_target)\n",
    "\n",
    "metrics.r2_score(wine_target, avg_quality * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(wine_target, (avg_quality + 1) * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Deeper Evaluation of Wine Data Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we could have investigated from our [model on the Wine Data](#Multiple-Regression-in-Scikit-Learn) is how our predictions $\\hat{y}$ match with the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_hat,kde=True,fill=False,stat='density',color='red')\n",
    "sns.histplot(wine_target,discrete=True,stat='density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's a slight issue with our model; the linear regression believes the target values are on a continuum. We know that's not true from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy fix is to round the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_rounded = np.round(y_hat)\n",
    "np.unique(y_hat_rounded, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution is a lot more meaningful if we require targets to be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.round(y_hat),fill=False,discrete=True,stat='density',color='red')\n",
    "sns.histplot(wine_target,discrete=True,alpha=0.3,stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our $R^2$ metric will be worse. This makes sense since we found a \"line of best fit\" that predicts continuous values. \n",
    "\n",
    "If the better option was _integer_ predictions, it would have predicted that instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(wine_target, y_hat_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must decide yourself if this is worth doing or if a different model makes more sense (we'll see more models in future lectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Regression with Categorical Features with the Comma Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll try to predict the first column of df: the extent to which\n",
    "# the person accepts the sentence\n",
    "# without the Oxford comma as more grammatically correct.\n",
    "\n",
    "comma_target = comma_df['x0_It\\'s important for a person to be honest, kind, and loyal.']\n",
    "\n",
    "comma_predictors = comma_df[['x8_30-44',\n",
    "       'x8_45-60', 'x8_> 60', 'x9_$100,000 - $149,999',\n",
    "       'x9_$150,000+', 'x9_$25,000 - $49,999', 'x9_$50,000 - $99,999']]\n",
    "\n",
    "comma_lr = LinearRegression()\n",
    "\n",
    "comma_lr.fit(comma_predictors, comma_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_lr.score(comma_predictors, comma_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_preds = comma_lr.predict(comma_predictors)\n",
    "metrics.mean_squared_error(comma_target, comma_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_df.corr()['x0_It\\'s important for a person to be honest, kind, and loyal.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on the interpretation of regression coefficients for categorical variables, see [Erin's repo](https://github.com/hoffm386/coefficients-of-dropped-categorical-variables)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
