{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Simple-Linear-Regression\" data-toc-modified-id=\"Simple-Linear-Regression-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Simple Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Covariance-and-Correlation\" data-toc-modified-id=\"Covariance-and-Correlation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Covariance and Correlation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Covariance\" data-toc-modified-id=\"Covariance-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Covariance</a></span></li><li><span><a href=\"#Correlation\" data-toc-modified-id=\"Correlation-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Correlation</a></span></li></ul></li><li><span><a href=\"#Causation\" data-toc-modified-id=\"Causation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Causation</a></span></li><li><span><a href=\"#Statistical-Learning-Theory\" data-toc-modified-id=\"Statistical-Learning-Theory-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Statistical Learning Theory</a></span></li><li><span><a href=\"#Regression-Equation\" data-toc-modified-id=\"Regression-Equation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Regression Equation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Proof\" data-toc-modified-id=\"Proof-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Proof</a></span></li></ul></li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Interpretation</a></span></li><li><span><a href=\"#Using-best_line()\" data-toc-modified-id=\"Using-best_line()-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Using <code>best_line()</code></a></span></li></ul></li><li><span><a href=\"#Simple-Linear-Regression-with-statsmodels\" data-toc-modified-id=\"Simple-Linear-Regression-with-statsmodels-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Simple Linear Regression with <code>statsmodels</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Regression-Without-Error-in-statsmodels\" data-toc-modified-id=\"Regression-Without-Error-in-statsmodels-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Regression Without Error in <code>statsmodels</code></a></span></li><li><span><a href=\"#Regression-with-Error-in-statsmodels\" data-toc-modified-id=\"Regression-with-Error-in-statsmodels-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Regression with Error in <code>statsmodels</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Fitted-Model-Attributes-and-Methods\" data-toc-modified-id=\"Fitted-Model-Attributes-and-Methods-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Fitted Model Attributes and Methods</a></span></li></ul></li><li><span><a href=\"#Coefficient-of-Determination\" data-toc-modified-id=\"Coefficient-of-Determination-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Coefficient of Determination</a></span></li><li><span><a href=\"#Other-Regression-Statistics\" data-toc-modified-id=\"Other-Regression-Statistics-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Other Regression Statistics</a></span></li></ul></li><li><span><a href=\"#Assumptions-of-Linear-Regression\" data-toc-modified-id=\"Assumptions-of-Linear-Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Assumptions of Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linearity\" data-toc-modified-id=\"Linearity-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Linearity</a></span></li><li><span><a href=\"#Independence\" data-toc-modified-id=\"Independence-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Independence</a></span></li><li><span><a href=\"#Normality\" data-toc-modified-id=\"Normality-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Normality</a></span></li><li><span><a href=\"#Homoskedasticity\" data-toc-modified-id=\"Homoskedasticity-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Homoskedasticity</a></span></li><li><span><a href=\"#Violations-of-Assumptions\" data-toc-modified-id=\"Violations-of-Assumptions-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Violations of Assumptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Log-Scaling\" data-toc-modified-id=\"Log-Scaling-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Log Scaling</a></span></li></ul></li></ul></li><li><span><a href=\"#Level-Up:--Anscombe's-Quartet\" data-toc-modified-id=\"Level-Up:--Anscombe's-Quartet-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Level Up:  <a href=\"https://www.desmos.com/calculator/paknt6oneh\" target=\"_blank\">Anscombe's Quartet</a></a></span></li><li><span><a href=\"#Level-Up:-.sm.formula.ols()\" data-toc-modified-id=\"Level-Up:-.sm.formula.ols()-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Level Up: <code>.sm.formula.ols()</code></a></span></li><li><span><a href=\"#Level-Up:-Visualization-of-Error\" data-toc-modified-id=\"Level-Up:-Visualization-of-Error-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Level Up: Visualization of Error</a></span></li><li><span><a href=\"#Level-Up:-Adjusted-$R^2$\" data-toc-modified-id=\"Level-Up:-Adjusted-$R^2$-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Level Up: Adjusted $R^2$</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from random import gauss\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mpl_toolkits import mplot3d\n",
    "import sklearn.metrics as metrics\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.pardir)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from lin_reg import best_line\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain and use the concepts of covariance and correlation\n",
    "- Explain how to interpret linear regressions\n",
    "- Describe the assumptions of linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of _correlation_ is the simple idea that variables often change _together_. For a simple example, cities with more buses tend to have higher populations.\n",
    "\n",
    "We might observe that, as one variable X increases, so does another Y, OR that as X increases, Y decreases.\n",
    "\n",
    "The _covariance_ describes how two variables co-vary. Note the similarity in the definition to the definition of ordinary variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$\\Large\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$ <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 3, 5]\n",
    "Y = [2, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance by hand:\n",
    "((1-3) * (2-7) + (3-3) * (9-7) + (5-3) * (10-7)) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better yet: With NumPy:\n",
    "np.cov(X, Y, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to intepret a covariance matrix\n",
    "\n",
    "<img src=\"images/covariance-matrix.jpeg\" width=\"550\"> \n",
    "\n",
    "[Source](https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1#:~:text=It%20is%20a%20symmetric%20matrix,data%20spread%20among%20two%20dimensions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the value of the covariance is very much a function of the values of X and Y, which can make interpretation difficult. What is wanted is a _standardized_ scale for covariance, hence: _correlation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson Correlation:<br/>$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the $n$'s cancel!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf{Check}$:\n",
    "\n",
    "<details><summary>\n",
    "What happens if X = Y?\n",
    "</summary>\n",
    "Then numerator = denominator and the correlation = 1!\n",
    "</details>\n",
    "<br/>\n",
    "We'll always have $-1 \\leq r \\leq 1$. (This was the point of standardizing by the standard deviations of X and Y.)\n",
    "\n",
    "A correlation of -1 means that X and Y are perfectly negatively correlated, and a correlation of 1 means that X and Y are perfectly positively correlated.\n",
    "\n",
    "NumPy also has a correlation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 / np.sqrt(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)[0, 1] == (np.cov(X, Y, ddof=0) / (np.std(X) * np.std(Y)))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so does SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(X, Y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient [Interpretation](https://journals.lww.com/anesthesia-analgesia/fulltext/2018/05000/correlation_coefficients__appropriate_use_and.50.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Why_ does it happen that variables correlate? It _may_ be that one is the cause of the other. A city having a high population, for example, probably does have some causal effect on the number of buses that the city has. But this _need not_ be the case, and that is why statisticians are fond of saying that 'correlation is not causation'. An alternative possibility, for example, is that high values of X and Y are _both_ caused by high values of some third factor Z. The size of children's feet, for example, is correlated with their ability to spell, but this is of course NOT because either is a cause of the other. Rather, BOTH are caused by the natural maturing and development of children. As they get older, both their feet and their spelling abilities grow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important at this point to understand the distinction between dependent and independent variables.\n",
    "\n",
    "Roughly, the independent variable is what can be directly manipulated and the dependent variable is what cannot be (but is nevertheless of great interest). What matters structurally is simply that we understand the dependent variable to be a _function_ of the independent variable(s).\n",
    "\n",
    "This is the proper interpretation of a statistical _model_.\n",
    "\n",
    "Simple idea: We can model correlation with a _line_. As one variable changes, so does the other.\n",
    "\n",
    "This model has two *parameters*: *slope* and *y-intercept*.\n",
    "\n",
    "Unless there's a perfectly (and suspiciously) linear relationship between our predictor(s) and our target, there will  be some sort of **error** or **loss** or **residual**. The best-fit line is constructed by minimizing the sum of the squares of these losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "- slope: <br/>$\\Large m = r_P\\frac{\\sigma_y}{\\sigma_x} = \\frac{cov(X, Y)}{var(X)}$\n",
    "\n",
    "- y-intercept:<br/> $\\Large b = \\mu_y - m\\mu_x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details>\n",
    "    <summary>Click here</summary>\n",
    "\n",
    "We demonstrate this by setting the derivative of the loss function, $\\Sigma^n_{i=1}(y_i - (mx_i + b))^2$, equal to 0. **We shall see this calculus trick many times!**\n",
    "\n",
    "For this purpose we consider the loss a function of its optimizing parameters $m$ and $b$. So there are therefore two partial derivatives to consider. (We'll cover this in more depth later in the course.)\n",
    "\n",
    "(i) $\\frac{\\partial}{\\partial b}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}(y_i - mx_i - b)$\n",
    "\n",
    "(ii) $\\frac{\\partial}{\\partial m}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b)$\n",
    "\n",
    "- Let's set the first to 0:\n",
    "\n",
    "$-2\\sum^n_{i=1}(y_i - mx_i - b) = 0$ <br/>\n",
    "$\\sum^n_{i=1}(y_i - mx_i) = \\sum^n_{i=1}b = nb$ <br/>\n",
    "\n",
    "**So:** $\\large b = \\frac{\\sum^n_{i=1}(y_i - mx_i)}{n} = \\mu_y - m\\mu_x$\n",
    "\n",
    "- Let's set the second to 0:\n",
    "\n",
    "$-2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b) = 0$ <br/>\n",
    "$\\sum^n_{i=1}(x_iy_i - mx^2_i - bx_i) = 0$ <br/>\n",
    "\n",
    "- Plugging in our previous result, we have:\n",
    "\n",
    "$\\sum^n_{i=1}x_iy_i - (\\frac{1}{n}\\sum^n_{i=1}y_i - \\frac{m}{n}\\sum^n_{i=1}x_i)\\sum^n_{i=1}x_i - m\\sum^n_{i=1}x^2_i = 0$ <br/>\n",
    "$\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i + \\frac{m}{n}(\\sum^n_{i=1}x_i)^2 - m\\sum^n_{i=1}x^2_i = 0$ <br/>\n",
    "\n",
    "**So:** $\\large m = \\frac{\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i}{\\sum^n_{i=1}x^2_i - \\frac{1}{n}(\\sum^n_{i=1}x_i)^2} = \\frac{n\\times(\\frac{1}{n}\\sum^n_{i=1}x_iy_i - \\frac{1}{n^2}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i)}{n\\times(\\frac{1}{n}\\sum^n_{i=1}x^2_i - \\mu^2_x)} = \\frac{cov(X, Y)}{var(X)}$\n",
    "\n",
    "For more on the proof see [here](https://math.stackexchange.com/questions/716826/derivation-of-simple-linear-regression-parameters).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the simple linear regression algorithm is a pair of parameters: the slope and the y-intercept of the best-fit line through the data.\n",
    "\n",
    "***I therefore have a (more or less crude) MODEL of the phenomenon in question:***\n",
    "\n",
    "Suppose I have a bunch of data about (i) how many cigarettes people smoked in their lifetimes and (ii) how many years those same people lived. If I set my independent variable (\"x\") to be the number of cigarettes smoked and my dependent variable (\"y\") to be the number of years lived, then ***for any deceased person at all I will have a way of estimating the number of years that person lived if I know the number of cigarettes that that person smoked***. This estimate is exactly what the best-fit line gives me.\n",
    "\n",
    "Suppose the parameters of the regression come out to be $\\beta_0 = 100$ years and $\\beta_1 = -1\\times 10^{-4}$ years / cigarette ([in reality](https://www.medicalnewstoday.com/releases/9703#1) these are probably both a bit high).\n",
    "\n",
    "Then we would be modeling the lifespan of human beings according to the number of cigarettes smoked:\n",
    "\n",
    "$Y = \\beta_1\\times n + \\beta_0$,\n",
    "\n",
    "where $Y$ = the number of years (estimated) and $n$ is the number of cigarettes smoked.\n",
    "\n",
    "- If someone smoked 0 cigarettes, then we would estimate that person's lifespan as:\n",
    "\n",
    "$-1\\times 10^{-4}\\times 0 + 100 = 100$ years.\n",
    "\n",
    "- If someone smoked a pack a day for 30 years, that's 20 * 365 * 30 = 219000 cigarettes (never mind about leap years!), so we would estimate that person's lifespan as:\n",
    "\n",
    "$-1\\times 10^{-4}\\times 219000 + 100 = 78.1$ years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `best_line()`\n",
    "\n",
    "Let's take a look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_line(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-fit line exists no matter what my data look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rand = stats.uniform.rvs(size=100)\n",
    "Y_rand = stats.uniform.rvs(size=100)\n",
    "\n",
    "best_line(X_rand, Y_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: [Playing with regression line](https://www.desmos.com/calculator/jwquvmikhr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression with `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how to build a simple linear regression model with `statsmodels`. The `statsmodels` package offers a highly descriptive report of the fit of a regression model. Let's generate a simple regression and then analyze the report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try data that fit a straight line perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = 3*x + 5\n",
    "\n",
    "test_df = pd.DataFrame({'x': x, 'y':y})\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Without Error in `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodels` OLS class API takes an endogenous (dependent) variable and an exogenous (independent) variable. We also want an intercept term, so we'll use the `add_constant()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The data itself:\\n\", test_df['x'], '\\n', '*'*64)\n",
    "print(\"The data with an extra column of 1's:\\n\", sm.add_constant(test_df['x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense?\n",
    "\n",
    "Instead of setting up the regression $y$ ~ $x$, we're setting up $y$ ~ $x_1$ + $x_2$, where $x_2 = 1$ for all observations.\n",
    "\n",
    "- **Without** the constant, we're looking for a parameter $\\beta_1$ that minimizes the error around $y = \\beta_1x$;\n",
    "- **With** the constant, we're looking for two parameters $\\beta_0$ and $\\beta_1$ that minimize the error around $y = \\beta_1x_1 + \\beta_0x_2 = \\beta_1x_1 + \\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = test_df['y']\n",
    "exog = sm.add_constant(test_df['x'])\n",
    "lin_reg_model = sm.OLS(endog, exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call the `.summary()` method on the fitted model object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodel` summary table, and [more info](https://www.geeksforgeeks.org/interpreting-the-results-of-linear-regression-using-ols-summary/) on how to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Error in `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a little noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = np.array([3*pt + 5 + gauss(mu=0, sigma=5) for pt in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitted Model Attributes and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model has [many](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html) attributes and methods. I'll look at a couple here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mse_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.summary()` method contains lots of helpful information about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all these statistics!? Let's say a word about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often a data scientist will calculate $R^2$, the *coefficient of determination*, as a measure of how well the model fits the data.\n",
    "\n",
    "$R^2$ for a model is ultimately a _relational_ notion. It's a measure of goodness of fit _relative_ to a (bad) baseline model. This bad baseline model is simply the horizontal line $y = \\mu_Y$, for dependent variable $Y$.\n",
    "\n",
    "The actual calculation of $R^2$ is: <br/> $\\Large R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$.\n",
    "\n",
    "$R^2$ is a measure of how much variation in the dependent variable your model explains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Regression Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else do we have in this report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **F-statistic**: The F-test measures the significance of your model relative to a model in which all coefficients are 0, i.e. relative to a model that says there is no correlation whatever between the predictors and the target. <br/><br/>\n",
    "- **Log-Likelihood**: The probability in question is the probability of seeing these data points, *given* the model parameter values. The higher this is, the more our data conform to our model and so the better our fit. AIC and BIC are related to the log-likelihood; we'll talk about those later. <br/><br/>\n",
    "- **coef**: These are the betas as calculated by the least-squares regression. We also have p-values and 95%-confidence intervals. <br/><br/>\n",
    "- **Omnibus**: This is a test for error normality. The probability is the chance that the errors are normally distributed. <br/><br/>\n",
    "- **Durbin-Watson**: This is a test for autocorrelation. We'll return to this topic in a future lecture. <br/><br/>\n",
    "- **Jarque-Bera**: This is another test for error normality. <br/><br/>\n",
    "- **Cond. No.**: The condition number tests for independence of the predictors. Lower scores are better. When the predictors are *not* independent, we can run into problems of multicollinearity. For more on the condition number, see [here](https://stats.stackexchange.com/questions/168259/how-do-you-interpret-the-condition-number-of-a-correlation-matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many good regression diagnostics are available in** [`statsmodels`](https://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html). For more on statsmodels regression statistics, see [here](https://www.accelebrate.com/blog/interpreting-results-from-linear-regression-is-the-data-appropriate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression models make the following assumptions about the data and the process that generated them.\n",
    "\n",
    "[Here](https://www.statisticssolutions.com/assumptions-of-linear-regression/) is a helpful resource on the assumptions of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.plot_regress_exog(model, 'x1', fig=plt.figure(figsize=(12, 8)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The relationship between the target and predictor is linear.** Check this by drawing a scatter plot of your predictor and your target, and see if there is evidence that the relationship might not follow a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The errors are independent**. In other words: Knowing the error for one point doesn't tell you anything about the error for another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can I check for this?**\n",
    "- Make a scatter plot of the residuals and target values and look for patterns\n",
    "- Check the Durbin-Watson score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The errors are normally distributed.** That is, smaller errors are more probable than larger errors, according to the familiar bell curve.\n",
    "\n",
    "**How can I check for this?**\n",
    "- Make a histogram of the residuals\n",
    "- Build a QQ-Plot\n",
    "- Check the Jarque-Bera or Omnibus p-value (from `statsmodels` output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homoskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The errors are homoskedastic.** That is, the errors have the same variance. \n",
    "\n",
    "(The Greek word $\\sigma\\kappa\\epsilon\\delta\\acute{\\alpha}\\nu\\nu\\upsilon\\mu\\iota$ means \"to scatter\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can I check for this?**\n",
    "- Make a scatter plot of the residuals and target values and look to see if they are more or less spread out at different places\n",
    "- Conduct a formal test (e.g. Goldfeld-Quandt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violations of Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No model is perfect, and your assumptions will never hold perfectly. If the violations of assumptions are severe, you can try adjusting the data so the assumptions will hold, such as by... \n",
    "\n",
    "- Transforming your data with a non-linear function (e.g. log)\n",
    "- Only modeling a subset of your data\n",
    "- Dropping outliers\n",
    "\n",
    "These can make it harder to explain or interpret your model, but the trade-off may be worth it. Alternatively, you may be better of just using a different type of model (you will learn many)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no assumption that the predictor and the target *themselves* be normally distributed. However, linear regression can work better if the predictor and target are normally distributed. \n",
    "\n",
    "Log-scaling can be a good tool to make right-skewed data more normal.\n",
    "\n",
    "Suppose e.g. a kde plot of my predictor $X$ looks like this:\n",
    "\n",
    "![original](./images/skewplot.png)\n",
    "\n",
    "In that case, the kde plot of a log-transformed version of $X$ could look like this:\n",
    "\n",
    "![log](./images/logplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a problem like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "pred = stats.norm().rvs(2000)\n",
    "y = stats.expon().rvs(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(y, sm.add_constant(pred)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(np.log(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if I now build a model with this transformed target, how do I interpret my LR coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(np.log(y), sm.add_constant(pred)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the transformation, I would have said that a one-unit increase in x results on average in a 0.03 increase in y. But what I need to say now is that a one-unit increase in x results on average in a 0.03 increase *in the logarithm of y*, i.e. an increase in y by a factor of $e^{0.03}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "A one-unit increase in the x-variable corresponds\n",
    "to an increase in y by a factor of {round(np.exp(0.03), 3)},\n",
    "or {round(np.exp(0.03) - 1, 3)}%.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up:  [Anscombe's Quartet](https://www.desmos.com/calculator/paknt6oneh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we care about all these assumption checks? They let's us know if we've run a linear regression when we shouldn't have. Anscombe's Quartet demonstates this by showing four sets of data that are wildly different and problematic, but produce the same regression line.\n",
    "\n",
    "The [Wikipedia](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) article has even more information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = sns.load_dataset('anscombe')\n",
    "sns.scatterplot(data=ans, x='x', y='y', hue='dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Up: `.sm.formula.ols()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is an alternative way of using `statsmodels` to set up a linear regression, and that is with `sm.formula.ols()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.formula.ols(formula=\"y ~ x\", data=test_df).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that this way of doing things *automatically sets up an intercept column*.\n",
    "\n",
    "That's an advantage, but this method has drawbacks inasmuch as the value for the `data` parameter has to have a certain structure. And in fact, once we start using train-test splits, our data will often *fail* to have that requisite structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Visualization of Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting X so that the intercept term of the best-fit line will be 0\n",
    "X = np.array([1.5, 3.5, 5.5])\n",
    "Y = np.array([2, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X.reshape(-1, 1), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(m):\n",
    "    # sum of squared errors\n",
    "    line = m*X\n",
    "    err = sum(x**2 for x in [line - model.predict(X.reshape(-1, 1))])\n",
    "    return sum(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ms = np.linspace(0, 5, 100)\n",
    "ys = [sse(m) for m in ms]\n",
    "\n",
    "ax.set_xlabel('Slope Estimates')\n",
    "ax.set_ylabel('Sum of Squared Errors')\n",
    "ax.plot(ms, ys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going 3d to plot error as a function of both m and b\n",
    "\n",
    "def new_sse(m, x, b, y):\n",
    "    \"\"\"\n",
    "    This function returns the sum of squared errors for\n",
    "    a target y and a linear estimate mx + b.\n",
    "    \"\"\"\n",
    "    return len(x) * metrics.mean_squared_error(y, m*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going back to our original example\n",
    "X_sample = np.array([1, 3, 5])\n",
    "Y_sample = np.array([2, 9, 10])\n",
    "\n",
    "# This should be our minimum error\n",
    "new_sse(2, X_sample, 1, Y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.linspace(-3, 7, 100)\n",
    "bs = np.linspace(-5, 5, 100)\n",
    "\n",
    "X_grid, Y_grid = np.meshgrid(ms, bs)\n",
    "\n",
    "Z = np.array([[new_sse(m, X_sample, b, Y_sample) for m in ms] for b in bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_errs = {}\n",
    "for m in ms:\n",
    "    m_errs[m] = new_sse(m, X_sample, 1, Y_sample)\n",
    "print(min(m_errs.values()))\n",
    "for k in m_errs:\n",
    "    if m_errs[k] == min(m_errs.values()):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_errs = {}\n",
    "for b in bs:\n",
    "    b_errs[b] = new_sse(2, X_sample, b, Y_sample)\n",
    "print(min(b_errs.values()))\n",
    "for k in b_errs:\n",
    "    if b_errs[k] == min(b_errs.values()):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X_grid, Y_grid, Z)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');\n",
    "plt.savefig('images/surfacePlotSSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X_grid, Y_grid, Z, 200)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');\n",
    "plt.savefig('images/contourPlotSSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some theoretical [objections](https://data.library.virginia.edu/is-r-squared-useless/) to using $R^2$ as an evaluator of a regression model.\n",
    "\n",
    "One objection is that, if we add another predictor to our model, $R^2$ can only *increase*! (It could hardly be that with more features I'd be able to account for *less* of the variation in the dependent variable than I could with the smaller set of features.)\n",
    "\n",
    "One improvement is **adjusted $R^2$**: <br/> $\\Large R^2_{adj.}\\equiv 1 - \\frac{(1 - R^2)(n - 1)}{n - m - 1}$, where:\n",
    "\n",
    "- n is the number of data points; and\n",
    "- m is the number of predictors.\n",
    "\n",
    "This can be a better indicator of the quality of a regression model. For more, see [here](https://towardsdatascience.com/demystifying-r-squared-and-adjusted-r-squared-52903c006a60).\n",
    "\n",
    "Note that Adjusted $R^2$ *can* be negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
